{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-lodge",
   "metadata": {},
   "source": [
    "# QuickTorch Playground\n",
    "https://neptune.ai/blog/pytorch-loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "positive-priority",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "constitutional-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-dependence",
   "metadata": {},
   "source": [
    "## The Mean Absolute Error\n",
    "- Regression problems, especially when the distribution of the target variable has outliers, \n",
    "  - such as small or big values that are a great distance from the mean value. It is considered to be more robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "satisfied-american",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-2.6039, -1.2204,  0.6570,  0.0422,  0.3718],\n",
      "        [-0.6171,  1.1668,  0.6107, -1.2699,  0.0993],\n",
      "        [-1.5890, -1.7168,  0.5738,  1.3962,  0.1433]], requires_grad=True)\n",
      "target:  tensor([[ 0.1537, -0.9205,  1.2392, -0.0545,  0.3506],\n",
      "        [ 0.2091,  1.1160,  2.1931,  1.5657,  1.9054],\n",
      "        [-2.0823, -0.8792, -0.5747, -1.5154,  1.3977]])\n",
      "output:  tensor(1.1669, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "mae_loss = nn.L1Loss()\n",
    "output = mae_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-healthcare",
   "metadata": {},
   "source": [
    "## Mean Squared Error Loss Function\n",
    "- MSE is the default loss function for most Pytorch regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "persistent-injection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-2.6039, -1.2204,  0.6570,  0.0422,  0.3718],\n",
      "        [-0.6171,  1.1668,  0.6107, -1.2699,  0.0993],\n",
      "        [-1.5890, -1.7168,  0.5738,  1.3962,  0.1433]], requires_grad=True)\n",
      "target:  tensor([[ 0.1537, -0.9205,  1.2392, -0.0545,  0.3506],\n",
      "        [ 0.2091,  1.1160,  2.1931,  1.5657,  1.9054],\n",
      "        [-2.0823, -0.8792, -0.5747, -1.5154,  1.3977]])\n",
      "output:  tensor(2.3233, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "output = mse_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-stranger",
   "metadata": {},
   "source": [
    "## Negative Log-Likelihood Loss Function\n",
    "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "\n",
    "- Multi-class classification problems\n",
    "\n",
    "<img src=\"https://i.imgur.com/hU252jE.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "demographic-aquarium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-2.6039, -1.2204,  0.6570,  0.0422,  0.3718],\n",
      "        [-0.6171,  1.1668,  0.6107, -1.2699,  0.0993],\n",
      "        [-1.5890, -1.7168,  0.5738,  1.3962,  0.1433]], requires_grad=True)\n",
      "target:  tensor([2, 4, 4])\n",
      "output:  tensor(1.5350, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# size of input (N x C) is = 3 x 5\n",
    "# every element in target should have 0 <= value < C\n",
    "# target_2 = torch.tensor([1, 0, 4])\n",
    "target_2 = torch.tensor([2, 4, 4])\n",
    "\n",
    "# (NLL) is applied only on models with the softmax function as an output activation layer. \n",
    "# Softmax refers to an activation function that calculates the normalized exponential function of every unit in the layer.\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "nll_loss = nn.NLLLoss()\n",
    "output = nll_loss(m(input), target_2)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target_2)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-sight",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss Function\n",
    "- Common type is the Binary Cross-Entropy (BCE)\n",
    "  - The BCE Loss is mainly used for binary classification models\n",
    "- Creating confident modelsâ€”the prediction will be accurate and with a higher probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indian-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-2.6039, -1.2204,  0.6570,  0.0422,  0.3718],\n",
      "        [-0.6171,  1.1668,  0.6107, -1.2699,  0.0993],\n",
      "        [-1.5890, -1.7168,  0.5738,  1.3962,  0.1433]], requires_grad=True)\n",
      "target:  tensor([3, 3, 0])\n",
      "output:  tensor(2.7738, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target_2 = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "output = cross_entropy_loss(input, target_2)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target_2)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-school",
   "metadata": {},
   "source": [
    "## Hinge Embedding Loss Function\n",
    "- Classification problems, especially when determining if two inputs are dissimilar or similar. \n",
    "- Learning nonlinear embeddings or semi-supervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sporting-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-2.6039, -1.2204,  0.6570,  0.0422,  0.3718],\n",
      "        [-0.6171,  1.1668,  0.6107, -1.2699,  0.0993],\n",
      "        [-1.5890, -1.7168,  0.5738,  1.3962,  0.1433]], requires_grad=True)\n",
      "target:  tensor([[ 0.1537, -0.9205,  1.2392, -0.0545,  0.3506],\n",
      "        [ 0.2091,  1.1160,  2.1931,  1.5657,  1.9054],\n",
      "        [-2.0823, -0.8792, -0.5747, -1.5154,  1.3977]])\n",
      "output:  tensor(1.0375, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hinge_loss = nn.HingeEmbeddingLoss()\n",
    "output = hinge_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-benchmark",
   "metadata": {},
   "source": [
    "## Margin Ranking Loss Function\n",
    "- Ranking problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "existing-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input one:  tensor([-2.3125, -0.8794, -0.1077], requires_grad=True)\n",
      "input two:  tensor([ 0.7138,  0.8160, -2.4854], requires_grad=True)\n",
      "target:  tensor([ 1., -1.,  1.])\n",
      "output:  tensor(1.0088, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_one = torch.randn(3, requires_grad=True)\n",
    "input_two = torch.randn(3, requires_grad=True)\n",
    "target_3 = torch.randn(3).sign()\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target_3)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target_3)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-smoke",
   "metadata": {},
   "source": [
    "## Triplet Margin Loss Function\n",
    "- Determining the relative similarity existing between samples. \n",
    "- It is used in content-based retrieval problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "curious-agriculture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor:  tensor([[-0.7325, -1.7829, -0.4846,  ..., -0.0598, -0.1298, -0.1603],\n",
      "        [ 0.0557, -0.5592, -1.6508,  ...,  0.9345, -0.9865,  1.0287],\n",
      "        [-0.4330, -0.8617,  0.9119,  ..., -0.5017,  1.0565, -1.7936],\n",
      "        ...,\n",
      "        [ 0.9324, -0.6961, -1.2872,  ...,  0.9515, -0.9232,  1.5767],\n",
      "        [-0.5145,  1.6347, -0.4723,  ...,  0.4561, -1.0959, -0.5120],\n",
      "        [ 1.4085,  0.1191,  0.7920,  ..., -0.0760,  0.6158,  0.9418]],\n",
      "       requires_grad=True)\n",
      "positive:  tensor([[ 0.3188,  0.1526, -2.4767,  ...,  1.2028, -1.2440,  1.5630],\n",
      "        [ 0.9972, -0.5618,  0.7896,  ...,  0.6141, -2.1275, -0.0886],\n",
      "        [-0.8415, -0.6662,  1.8936,  ...,  0.1795,  0.1603, -0.1437],\n",
      "        ...,\n",
      "        [ 1.1995, -0.5566,  1.1635,  ..., -0.5103, -0.2374,  0.0360],\n",
      "        [ 1.2933,  0.9803, -1.3911,  ...,  0.9058,  1.0857, -0.8077],\n",
      "        [-0.7215,  0.4894, -1.4079,  ...,  0.9180,  1.2208, -0.5951]],\n",
      "       requires_grad=True)\n",
      "negative:  tensor([[-0.0462,  0.9133, -0.4002,  ...,  0.7327, -0.0164, -0.2079],\n",
      "        [-1.7883, -0.3799, -0.0504,  ...,  0.1387, -0.7919, -0.1970],\n",
      "        [-0.1038, -0.7759, -1.0996,  ...,  0.0475,  0.3014, -0.2857],\n",
      "        ...,\n",
      "        [-2.2676,  1.2990,  0.3772,  ...,  0.9930, -0.8582,  0.5917],\n",
      "        [-0.2452,  1.6299,  1.7080,  ...,  1.6402, -0.2664,  0.8276],\n",
      "        [ 0.5951,  0.4969,  1.0503,  ...,  1.1525, -0.2094, -2.2395]],\n",
      "       requires_grad=True)\n",
      "output:  tensor(1.0559, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "\n",
    "triplet_margin_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "output = triplet_margin_loss(anchor, positive, negative)\n",
    "output.backward()\n",
    "\n",
    "print('anchor: ', anchor)\n",
    "print('positive: ', positive)\n",
    "print('negative: ', negative)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-boards",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence Loss Function\n",
    "- Approximating complex functions\n",
    "- Multi-class classification tasks\n",
    "- If you want to make sure that the distribution of predictions is similar to that of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respiratory-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-0.9284,  0.3639, -0.2313],\n",
      "        [ 0.5626, -1.0062, -0.9481]], requires_grad=True)\n",
      "target:  tensor([[-0.8218, -0.8638,  0.9639],\n",
      "        [-0.6536, -1.5656,  0.7465]])\n",
      "output:  tensor(0.3385, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 3, requires_grad=True)\n",
    "target_4 = torch.randn(2, 3)\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "output = kl_loss(input, target_4)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target_4)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-simple",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type 1\n",
    "def myCustomLoss(my_outputs, my_labels):\n",
    "    #specifying the batch size\n",
    "    my_batch_size = my_outputs.size()[0] \n",
    "    #calculating the log of softmax values           \n",
    "    my_outputs = F.log_softmax(my_outputs, dim=1)  \n",
    "    #selecting the values that correspond to labels\n",
    "    my_outputs = my_outputs[range(my_batch_size), my_labels] \n",
    "    #returning the results\n",
    "    return -torch.sum(my_outputs)/number_examples\n",
    "\n",
    "# type 2\n",
    "# in binary classification problem\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    " \n",
    "    def forward(self, inputs, targets, smooth=1):        \n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"0x05 CNN.ipynb","provenance":[],"collapsed_sections":["UjqVeaQYNik3","WLj3JMLgNilO"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"8MSWqw_XNik2","colab_type":"text"},"source":["# CNN\n","\n","https://pytorch.org/docs/stable/nn.html?highlight=maxpool2d#torch.nn.MaxPool2d\n","\n","https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d\n"]},{"cell_type":"markdown","metadata":{"id":"UjqVeaQYNik3","colab_type":"text"},"source":["## basic example"]},{"cell_type":"code","metadata":{"id":"5j7WpHlyNik4","colab_type":"code","colab":{}},"source":["# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLDjvxvWNik7","colab_type":"code","colab":{}},"source":["# Training settings\n","batch_size = 64\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sr60tD7sNilA","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.mp = nn.MaxPool2d(2)  # kernel size\n","        self.fc = nn.Linear(320, 10)  # avoid size mismatch, in: 64 * 320\n","\n","    def forward(self, x):\n","        in_size = x.size(0)\n","        x = F.relu(self.mp(self.conv1(x)))  # max pool, print x.size() to check out\n","        x = F.relu(self.mp(self.conv2(x)))\n","        x = x.view(in_size, -1)  # flatten the tensor\n","        x = self.fc(x)\n","        # It is almost always you will need the last dimension when you compute the cross-entropy\n","        return F.log_softmax(x, -1)  # add dim -1 to avoid warning...\n","\n","\n","model = Net()\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UL737dcZNilF","colab_type":"code","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data), Variable(target)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","# https://discuss.pytorch.org/t/two-small-questions-about-with-torch-no-grad/27571\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = Variable(data), Variable(target)  # change Variable(data, volatile=True)\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += F.nll_loss(output, target, size_average=False).data\n","        # get the index of the max log-probability\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhJHCXtbNilJ","colab_type":"code","colab":{},"outputId":"a11b7035-11c0-48b9-82e2-2eaf02c8671d"},"source":["for epoch in range(1, 10):\n","    train(epoch)\n","    test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.059847\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.194853\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.057874\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.083139\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.073367\n","\n","Test set: Average loss: 0.0939, Accuracy: 9730/10000 (97%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.129999\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.144854\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.191636\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.069492\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.155353\n","\n","Test set: Average loss: 0.0745, Accuracy: 9758/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.132020\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.071940\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.172060\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.129315\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.113893\n","\n","Test set: Average loss: 0.0726, Accuracy: 9769/10000 (97%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.022187\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.052929\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.059229\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.152275\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.060212\n","\n","Test set: Average loss: 0.0637, Accuracy: 9796/10000 (97%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.105415\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.105495\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.024767\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.148450\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.022125\n","\n","Test set: Average loss: 0.0589, Accuracy: 9801/10000 (98%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.029692\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.086405\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.059128\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.037345\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.008930\n","\n","Test set: Average loss: 0.0599, Accuracy: 9803/10000 (98%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.055871\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.187382\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.130143\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.030888\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.041143\n","\n","Test set: Average loss: 0.0517, Accuracy: 9835/10000 (98%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.027717\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.069186\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013716\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.010030\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.069819\n","\n","Test set: Average loss: 0.0528, Accuracy: 9834/10000 (98%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018547\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.044696\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.122486\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.058019\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.027669\n","\n","Test set: Average loss: 0.0512, Accuracy: 9830/10000 (98%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WLj3JMLgNilO","colab_type":"text"},"source":["## advance example\n","\n","In the context of convolutional neural networks, kernel = filter = feature detector\n","\n","http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n","\n","https://www.youtube.com/watch?v=VxhSouuSZDY\n","\n","https://buzzrobot.com/whats-happening-inside-the-convolutional-neural-network-the-answer-is-convolution-2c22075dc68d\n","\n","https://docs.python.org/3/library/argparse.html"]},{"cell_type":"code","metadata":{"id":"LTTkr7b2NilP","colab_type":"code","colab":{}},"source":["# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","from __future__ import print_function\n","import argparse  # interesting package indeed\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DvccdVUNilS","colab_type":"code","colab":{}},"source":["# Training settings\n","batch_size = 64\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8t8AjqMINilW","colab_type":"code","colab":{}},"source":["# make comparison of each convolution\n","class InceptionA(nn.Module):\n","    def __init__(self, in_channels):\n","        super(InceptionA, self).__init__()\n","        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n","\n","        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n","        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n","\n","        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n","        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n","        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n","\n","        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n","\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","\n","        branch5x5 = self.branch5x5_1(x)\n","        branch5x5 = self.branch5x5_2(branch5x5)\n","\n","        branch3x3dbl = self.branch3x3dbl_1(x)\n","        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n","        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n","\n","        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n","        branch_pool = self.branch_pool(branch_pool)\n","\n","        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n","        return torch.cat(outputs, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4lnbfC9uNilb","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n","\n","        self.incept1 = InceptionA(in_channels=10)\n","        self.incept2 = InceptionA(in_channels=20)\n","\n","        self.mp = nn.MaxPool2d(2)\n","        self.fc = nn.Linear(1408, 10)\n","\n","\n","    def forward(self, x):\n","        in_size = x.size(0)\n","        x = F.relu(self.mp(self.conv1(x)))\n","        x = self.incept1(x)  # inceptions\n","        x = F.relu(self.mp(self.conv2(x)))\n","        x = self.incept2(x)\n","        x = x.view(in_size, -1)  # flatten the tensor\n","        x = self.fc(x)\n","        # Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","        return F.log_softmax(x, -1)\n","\n","\n","model = Net()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qz2UGeu0Nilf","colab_type":"code","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data), Variable(target)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data))\n","\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = Variable(data), Variable(target)\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += F.nll_loss(output, target, size_average=False).data\n","        # get the index of the max log-probability\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUW4op9dNili","colab_type":"code","colab":{},"outputId":"10275edd-f348-40fb-9095-836b4212e17d"},"source":["for epoch in range(1, 10):\n","    train(epoch)\n","    test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.321258\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.051745\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.210887\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.033756\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.062098\n"],"name":"stdout"},{"output_type":"stream","text":["/home/jian/miniconda3/envs/ptgpu/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0895, Accuracy: 9714/10000 (97%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.081887\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.198613\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.122356\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.120923\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.033388\n","\n","Test set: Average loss: 0.0791, Accuracy: 9754/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.055966\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.103866\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.036902\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.048040\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.288322\n","\n","Test set: Average loss: 0.0634, Accuracy: 9800/10000 (98%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.038738\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.077438\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.071596\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.026354\n"],"name":"stdout"}]}]}
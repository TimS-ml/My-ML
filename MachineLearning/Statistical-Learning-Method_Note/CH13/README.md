# CH13 无监督学习概论
![Hits](https://www.smirkcao.info/hit_gits/Lihang/CH13/README.md)

[TOC]

## 前言

### 章节目录

1. 无监督学习基本原理
1. 基本问题
1. 机器学习三要素
1. 无监督学习方法

### 导读

- 在这部分强调了**样本(实例)**，由特征向量组成。

- 无监督学习的基本问题是聚类，降维，概率估计，对应的输出是类别，转换，概率。

- 无监督学习的模型是$z=g_{\theta}(x)$(硬聚类)，条件概率分布$\color{red}P_{\theta}(z|x)$（软聚类）**或条件概率分布$\color{green}P_{\theta}(x|z)$**（概率模型估计）

- 软聚类可以看成是概率模型估计问题，根据贝叶斯公式
  $$
  \color{red}P_\theta(z|x)\color{black}=\frac{P_\theta(x|z)P_\theta(z)}{P_\theta(x)}\varpropto \underbrace{P_\theta(z)}_{假设服从均匀分布}P_\theta(x|z)\varpropto \color{green}P_\theta(x|z)
  $$
  假设先验概率服从均匀分布，$P_\theta(z)$就是常数，先验后验成正比。

- 训练数据可以用$M\times N$矩阵表示，注意这里矩阵的每一行对应特征，每一列对应一个样本，回想在之前监督学习部分，也是用$N$表示样本的个数。但是这里面其实稍微有一点点尴尬，在监督学习部分，特征的维数用$n$表示，样本数量用$N$表示，在无监督学习部分，特征的维数用$m$表示，样本的数量用$n$表示。

- 无监督学习可以用于数据分析或者监督学习的前处理

- `无监督学习通常需要大量的数据，因为对数据隐藏的规律的发现需要足够的观测`。反过来看，当我们拥有大量数据的时候，可以考虑通过无监督学习的方式来发现数据中隐藏的规律。有的时候我们需要从问题的角度出发来寻找解决方案，而有的时候，我们需要从自身的角度出发，来看能做些什么。不同的岗位看问题的角度不同，但是岗位的差异并不应该限制你思考问题的维度。

- 这章的参考文献和第一章的基本一样，去掉了Sutton的强化学习。

- 降维部分有提到manifold，这个书中应该是没有展开，可以参考scikit-learn中[相关部分](https://scikit-learn.org/stable/modules/manifold.html#manifold)。

## 无监督学习基本原理

符号说明

训练数据集$X$
$$
X=\left[
\begin{matrix}
 x_{11} & \cdots & x_{1N}       \\
 \vdots &        & \vdots 		\\
 x_{M1} & \cdots & x_{MN}       \\
\end{matrix}
\right]
$$

## 基本问题

### 聚类

发现数据中的**纵向结构**

硬聚类
$$
z=g_\theta(x)
$$
软聚类
$$
P_\theta(z|x)
$$


$x\in X$是样本的向量

$z\in Z$是样本的类别

$\theta$是参数

### 降维

发现数据中的**横向结构**
$$
z=g_\theta(x)
$$
$x\in X$ 是样本的高维向量

$z\in Z$ 是样本的低维向量

$\theta$ 是参数

$g$ 可以是线性函数，也可以是非线性函数。

### 概率模型估计

$$
P_\theta(x|z)
$$

找到最有可能生成数据的结构和参数。

## 机器学习三要素

模型，策略，算法，这部分概括下和监督学习主要的区别在模型差异，至于策略和算法，大体路子和监督学习差不多，具体问题具体分析。

关于模型，上一小节有介绍，这一小节简单重复下，将在下一小节展开，并在后续章节详细说明。

关于策略，各自目标函数的优化过程，比如**聚类**样本与所属类别中心距离的最小化，**降维**过程信息损失的最小化，**概率模型估计**过程中生成数据概率的最大化。

关于算法，迭代算法，梯度下降打天下。

## 无监督学习方法

### 聚类

[CH14](../CH14/README.md)，主要是数据硬聚类，Kmeans，层次聚类，软聚类在之前的[EM算法](../CH09/README.md)部分已经讲过，比如高斯混合模型。

### 降维

[CH15](../CH15/README.md)，[CH16](../CH16/README.md)，主成分分析，以及奇异值分解。

这部分先写了第16章介绍PCA，然后说的第15章。因为PCA是无监督学习方法，SVD是基础学习方法。后面话题分析部分也是这样的顺序。

### 话题分析

LSA，PLSA，LDA是无监督学习方法，MCMC是基础的学习方法。

### 图分析

PageRank，Page是个人名。

发现隐藏在图中的统计规律或潜在结构。

## 参考


{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-08-03T05:52:12.616562Z","iopub.status.busy":"2022-08-03T05:52:12.615867Z","iopub.status.idle":"2022-08-03T05:52:12.67096Z","shell.execute_reply":"2022-08-03T05:52:12.669595Z","shell.execute_reply.started":"2022-08-03T05:52:12.616465Z"},"trusted":true},"outputs":[],"source":["%config IPCompleter.use_jedi = False\n","%config InlineBackend.figure_format='svg'\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-08-03T05:52:12.674819Z","iopub.status.busy":"2022-08-03T05:52:12.674329Z","iopub.status.idle":"2022-08-03T05:52:13.755698Z","shell.execute_reply":"2022-08-03T05:52:13.754488Z","shell.execute_reply.started":"2022-08-03T05:52:12.674773Z"},"trusted":true},"outputs":[],"source":["import os\n","import scipy\n","import urllib\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from scipy.stats import norm, multivariate_normal\n","\n","import plotly\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","seed = 1234\n","np.random.seed(seed)\n","plt.style.use(\"seaborn\")"]},{"cell_type":"markdown","metadata":{},"source":["Hello Kagglers! This is going to be a series on understanding **Denoising Diffuison Probabilistic Models** or **DDPMs** for short. In this notebook I will talk about the **Gaussian Distribution** and its importance in understanding the **Denoising Diffusion Models**. Not only in the context of DDPMs but every Data Scientist/ML engineer should also be aware of Gaussian distribution and its implications. We will cover the following topics in this notebook:\n","\n","* **Univariate Normal Distribution**\n","* **Multivariate Normal**\n","* **Covariance and Covariance Matrix**\n","* **Isotropic Gaussian**\n","* **Conditional Gaussian**\n","* **Convolution of PDFs**\n","* **Forward process in DDPMs and how all of the above is related to it**\n","\n","\n","We will cover the maths behind these concepts and will code them along the way. IMO the maths looks much scarier when presented in equations, but it becomes simple once you look at the code implementation. So, don't be scared by the greek symbols used in the equations. Try to undersatnd the code. It will help you understand the concepts better. Also, most of the visualizations presented in this notebook are interactive. Without further ado, let's start!"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Normal Distribution\n","\n","Also known as **Gaussian** Distribution, the normal distribution is one of the most widely used continuous probability distributions. The notation, $\\mathcal{N}(\\mu, \\sigma^{2})$ , is commonly used to refer to a normal distribution. \n","\n","The parameter $\\mu$ represents the **mean** or **expected value** of the distribution, while the parameter $\\sigma$ is its **standard deviation**. The **variance** of the distribution is $\\sigma^{2}$\n","\n","For a real-valued random variable `x`, the probability density function is defined as:\n","$$\n","p(x; \\mu, \\sigma^{2}) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} exp({\\frac{-1}{2}(\\frac {x - \\mu}{\\sigma})^{2}}) \\tag {1}\n","$$\n","\n","\n","# 1.1 Univariate Normal Distribution\n","\n","When we deal with only one random variable, e.g. `x` in the above equation, we call the distribution a univariate normal distribution. Let's code up the above equation and take an example of a univariate normal distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:53:52.287865Z","iopub.status.busy":"2022-08-03T05:53:52.287406Z","iopub.status.idle":"2022-08-03T05:53:52.517903Z","shell.execute_reply":"2022-08-03T05:53:52.516707Z","shell.execute_reply.started":"2022-08-03T05:53:52.287827Z"},"trusted":true},"outputs":[],"source":["def get_univariate_normal(x, mu, sigma):\n","    \"\"\"Generates a PDF for a Univariate Normal Distribution.\n","\n","    Args:\n","        x: Vector of values sorted in ascending order\n","        mu: Mean of the Gaussian\n","        sigma: Standard Deviation of the Gaussian\n","    Returns:\n","        A PDF\n","    \"\"\"\n","    return 1 / (sigma * np.sqrt(2 * np.pi)) * (np.exp(-(x - mu)**2 / (2 * sigma**2)))\n","\n","\n","# Mean of the distribution\n","mu = 0.0\n","\n","# Standard deviation(SD for short)\n","sigma = 1.0\n","\n","# Generate a linspace for a random variable x\n","num_samples = 100\n","x = np.linspace(-3*sigma + mu, 3*sigma + mu, num=num_samples)\n","\n","# Plot the value against the PDF\n","fig = go.Figure(\n","    data=go.Scatter(\n","        x=x,\n","        y=get_univariate_normal(x, mu=mu, sigma=sigma),\n","        line=dict(width=3,color=\"black\"),\n","        fill=\"tonexty\",\n","        fillcolor=\"skyblue\",\n","    )\n",")\n","\n","fig.add_annotation(x=mu, y=-0.001, text=\"Mean\",showarrow=True, arrowhead=2)\n","fig.add_vline(x=mu, line_width=3, line_dash=\"dash\", line_color=\"green\")\n","fig.update_layout(\n","    {\n","        \"title\": {\n","            'text': f\"Univariate Gaussian Distribution <br> μ: {mu}, σ\\u00b2: {sigma**2}\",\n","            'y':0.95,\n","            'x':0.5,\n","            'xanchor': 'center',\n","            'yanchor': 'top',\n","            'font': dict(size=14)\n","        },\n","        \"xaxis\": {\"title\": \"X\"},\n","        \"yaxis\": {\"title\": \"Probability Density\"},\n","        \"margin\": dict(l=0, r=0, b=0, t=50)\n","    }\n",")\n","\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["Moving the mean value shifts the distribution from right to left while changing sigma affects the shape of the distribution. As the value of sigma increases, the curve gets more and more flat. Let's see that in action."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:54:02.858588Z","iopub.status.busy":"2022-08-03T05:54:02.857773Z","iopub.status.idle":"2022-08-03T05:54:03.407271Z","shell.execute_reply":"2022-08-03T05:54:03.406327Z","shell.execute_reply.started":"2022-08-03T05:54:02.858545Z"},"trusted":true},"outputs":[],"source":["# Moving the mean shifts the distribution to\n","# right/left while increasing/decresasing the\n","# variance makes the curve more flat/pointy respectively\n","# Let's take an example to see this in action\n","\n","# Random variable x\n","x = np.linspace(-10.0, 10.0, 100)\n","\n","# Combination of mu and sigma. The first value\n","# of any tuple represents mu while the second value\n","# represents sigma here.\n","mu_sigma_combos = [\n","    [(0, 1), (-3, 1), (3, 1)],\n","    [(0, 1), (0, 2), (0, 4)],\n","]\n","\n","# Line colors and widths to be used\n","# for different combinations\n","colors = [\"red\", \"black\", \"blue\"]\n","widths = [1, 2, 3]\n","subtitles = [\"Varying μ\", \"Varying σ\"]\n","\n","# Plot\n","_, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(12, 5), tight_layout=True)\n","for i, elem in enumerate(mu_sigma_combos):\n","    legend = []\n","    mus = set()\n","    for j, comb in enumerate(elem):\n","        mu, sigma = comb\n","        mus.add(mu)\n","        legend.append(f\"μ: {mu}, σ\\u00b2: {sigma**2}\")\n","        ax[i].plot(x, get_univariate_normal(x, mu, sigma), linewidth=widths[j], c=colors[j])\n","        ax[i].tick_params(labelbottom=True)\n","    \n","    ax[i].set_title(subtitles[i])\n","    ax[i].legend(legend, loc=\"upper right\")\n","    \n","    for mu in mus:\n","        ax[i].axvline(x=mu, color=\"green\", linestyle=\"--\")\n","\n","plt.suptitle(\"Univariate Normal Distribution\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 1.2 Multivariate Normal Distribution\n","\n","Multivariate normal distribution is the extension of univariate normal distribution to the case where we deal with a real-valued **vector input** instead of a single real-valued random variable. Like in the univariate case, the multivariate normal distribution has associated parameters $\\mu$ representing the **mean vector** and $\\Sigma$ representing the **covariance matrix**.\n","\n","This is the probability density function for multivariate case:\n","$$\n","p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2} \\ \\vert{\\Sigma}\\vert^{1/2}} \\ exp \\ (\\frac{-1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu))  \\tag{2}\n","$$\n","\n","* Random Variable $X = [x_{1}, \\ x_{2}, \\ x_{3}...]$ is a `D`-dimensional vector\n","* Mean $\\mu = [\\mu_{1}, \\ \\mu_{2}, \\ \\mu_{3}...]$ is a `D`-dimensional vector\n","* Covariance Matrix $\\Sigma$ is a `D X D` dimensional matrix\n","\n","\n","If the vector `X` is 2-dimensional, then that distribution is also known as **Bivariate Normal Distribution**.\n","\n","**Note:** We need two dimensions to visualize a univariate Gaussian, and three dimesnions to visualize a bivariate Gaussian. Hence for visualization purposes, we will show everything with a *bivariate* Gaussian, which then, can be extended to *multivariate* cases. Let's visualize a bivariate Gaussian. We will use `scipy.stats.multivariate_normal` to generate the probability density."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:54:44.623885Z","iopub.status.busy":"2022-08-03T05:54:44.623421Z","iopub.status.idle":"2022-08-03T05:54:44.73967Z","shell.execute_reply":"2022-08-03T05:54:44.738817Z","shell.execute_reply.started":"2022-08-03T05:54:44.623848Z"},"trusted":true},"outputs":[],"source":["def get_multivariate_normal(\n","    mu,\n","    cov,\n","    sample=True,\n","    sample_size=None,\n","    seed=None,\n","    gen_pdf=False,\n","    pos=None\n","):\n","    \"\"\"Builds a multivariate Gaussian Distribution.\n","    \n","    Given the mean vector and the covariance matrix,\n","    this function builds a multivariate Gaussian\n","    distribution. You can sample from this distribution,\n","    and generate probability density for given positions.\n","    \n","    Args:\n","        mu: Mean vector representing the mean values for\n","            the random variables\n","        cov: Covariance Matrix\n","        sample (bool): If sampling is required\n","        sample_size: Only applicable if sampling is required.\n","            Number of samples to extract from the distribution\n","        seed: Random seed to be passed for distribution, and sampling\n","        gen_pdf(bool): Whether to generate probability density\n","        pos: Applicable only if density is generated. Values for which\n","            density is generated\n","    Returns:\n","        1. A Multivariate distribution\n","        2. Sampled data if `sample` is set to True else `None`\n","        3. Probability Density if `gen_pdf` is set to True else `None`\n","    \"\"\"\n","    \n","    # 1. Multivariate distribution from given mu and cov\n","    dist = multivariate_normal(mean=mu, cov=cov, seed=seed)\n","    \n","    # 2. If sampling is required\n","    if sample:\n","        samples = dist.rvs(size=sample_size, random_state=seed)\n","    else:\n","        samples = None\n","    \n","    # 3. If density is required\n","    if gen_pdf:\n","        if pos is None:\n","            raise ValueError(\"`pos` is required for generating density!\")\n","        else:\n","            pdf = dist.pdf(pos)\n","    else:\n","        pdf = None\n","        \n","    return dist, samples, pdf\n","\n","\n","\n","# Mean of the random variables X1 and X2\n","mu_x1, mu_x2 = 0, 0\n","\n","# Standard deviation for the random variables X1 and X2\n","sigma_x1, sigma_x2 = 1, 1\n","\n","# Positions for which probability density is\n","# to be generated\n","x1, x2 = np.mgrid[\n","    (-3.0 * sigma_x1 + mu_x1):(3.0 * sigma_x1 + mu_x1): 0.1,\n","    (-3.0 * sigma_x2 + mu_x2):(3.0 * sigma_x2 + mu_x2): 0.1\n","]\n","pos = np.dstack((x1, x2))\n","\n","# Mean vector\n","mu = [mu_x1, mu_x2]\n","\n","# Covariance between the two random variables\n","cov_x1x2 = 0\n","\n","# Covariance Matrix for our bivariate Gaussian distribution\n","cov = [[sigma_x1**2, cov_x1x2], [cov_x1x2, sigma_x2**2]]\n","\n","# Build distribution generate density\n","sample = get_multivariate_normal(\n","        mu=mu,\n","        cov=cov,\n","        sample=False,\n","        seed=seed,\n","        gen_pdf=True,\n","        pos=pos\n","    )\n","\n","\n","# Plot the bivariate normal density\n","fig = go.Figure(\n","    go.Surface(\n","        z=sample[2],\n","        x=x1,\n","        y=x2,\n","        colorscale='Viridis',\n","        showscale=False\n","    )\n",")\n","fig.update_layout(\n","    {\n","        \"title\": dict(\n","            text=\"Bivariate Distribution\",\n","            y=0.95,\n","            x=0.5,\n","            xanchor=\"center\",\n","            yanchor=\"top\",\n","            font=dict(size=12)\n","        ), \n","        \"scene\":dict(\n","            xaxis=dict(title='x1'),\n","            yaxis=dict(title='x2'),\n","            zaxis=dict(title='Probability density')\n","        ),\n","        \"xaxis\": dict(title=\"Values\"),\n","        \"yaxis\": dict(title=\"Probability Density\"),\n","    }\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["That's an interactive plot. You can zoom in/out, and rotate the surface to see what the Gaussians look like. When you hover over the density, you will see black lines generating two univariate normals, one for `x1` and another for `x2`\n","\n","\n","# 2. Covariance\n","\n","In the last section, we introduced the term `covariance matrix`. Before we move on to the next section, it is important to understand the concept of covariance. \n","\n","Let's say `X` and `Y` are two random variables. Covariance of `X` and `Y` is defined as:\n","\n","$$\n","Cov(X, Y) = E[(X - E(X))(Y - E(Y))] = E[XY] - E[X]E[Y]  \\tag{3}\n","$$\n","\n","i.e. the covariance is the expected value of the product of their deviations from their individual expected values.\n","\n","\n","So, what does **covariance** tell us? A lot...\n","\n","1. Covariance gives a sense of how much two random variables as well their scales are **linearly** related\n","2. Covariance captures only linear dependence and gives no information about other kind of relationships\n","3. If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. If the sign of the covariance is negative, then one variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa.\n","4. If two variables `X` and `Y` are **independent**, then $Cov(X, Y)= 0$ but the reverse isn't true. Why? Because covariance doesn't take into account non-linear relationships\n","\n","# 2.1 Covariance Matrix\n","\n","If `X` is a `N`-dimensional vector i.e. $X = [x_{1}, \\ x_{2}, \\ x_{3}...x_{n}]$, the covariance matrix is a `N`X`N` matrix defined as:\n","\n","$$\n","\\begin{bmatrix}\n","Cov(x_{1}x_{1}) & \\cdots & Cov(x_{1}x_{n}) \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","Cov(x_{n}x_{1}) & \\cdots & Cov(x_{n}x_{n})\n","\\end{bmatrix}\n","$$\n","\n","Each entry $(i, j)$ in this matrix defines the covariance of two random variables of the vector. Also:\n","\n","$$\n","Cov(x_{i}x_{j}) = Var(x_{i}) \\ \\ \\ \\{ i=j\\}\n","$$\n","\n","Let's take an example to showcase zero covariance, positive covariance, and negative covariance between two random variables `x1` and `x2`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:55:00.799915Z","iopub.status.busy":"2022-08-03T05:55:00.798534Z","iopub.status.idle":"2022-08-03T05:55:01.626676Z","shell.execute_reply":"2022-08-03T05:55:01.625669Z","shell.execute_reply.started":"2022-08-03T05:55:00.799859Z"},"trusted":true},"outputs":[],"source":["# Mean of the random variables X1 and X2\n","mu_x1, mu_x2 = 0, 0\n","\n","# Standard deviation of the random variables X1 and X2\n","sigma_x1, sigma_x2 = 1, 1\n","\n","# Number of samples to extract\n","sample_size = 2000\n","\n","# Positions for which probability density is\n","# to be generated\n","x1, x2 = np.mgrid[\n","    (-3.0 * sigma_x1 + mu_x1):(3.0 * sigma_x1 + mu_x1): 0.1,\n","    (-3.0 * sigma_x2 + mu_x2):(3.0 * sigma_x2 + mu_x2): 0.1\n","]\n","pos = np.dstack((x1, x2))\n","\n","# Mean vector\n","mu = [mu_x1, mu_x2]\n","\n","\n","# Case 1: Zero Covariance\n","cov_x1x2 = 0\n","zero_cov = [[sigma_x1**2, cov_x1x2], [cov_x1x2, sigma_x2**2]]\n","# Build distribution, sample and generate density\n","zero_cov_res = get_multivariate_normal(\n","        mu=mu,\n","        cov=zero_cov,\n","        sample=True,\n","        sample_size=sample_size,\n","        seed=seed,\n","        gen_pdf=True,\n","        pos=pos\n","    )\n","\n","\n","# Case 2: Positive Covarinace\n","cov_x1x2 = 0.9\n","pos_cov = [[sigma_x1**2, cov_x1x2], [cov_x1x2, sigma_x2**2]]\n","# Build distribution, sample and generate density\n","pos_cov_res = get_multivariate_normal(\n","        mu=mu,\n","        cov=pos_cov,\n","        sample=True,\n","        sample_size=sample_size,\n","        seed=seed,\n","        gen_pdf=True,\n","        pos=pos\n","    )\n","\n","\n","# Case 3: Negative Covarinace\n","cov_x1x2 = -0.9\n","neg_cov = [[sigma_x1**2, cov_x1x2], [cov_x1x2, sigma_x2**2]]\n","# Build distribution, sample and generate density\n","neg_cov_res = get_multivariate_normal(\n","        mu=mu,\n","        cov=neg_cov,\n","        sample=True,\n","        sample_size=sample_size,\n","        seed=seed,\n","        gen_pdf=True,\n","        pos=pos\n","    )\n","\n","# Plot the covariances\n","_, ax = plt.subplots(1, 3, figsize=(10, 4), sharex=True, sharey=True)\n","samples = [neg_cov_res[1], zero_cov_res[1], pos_cov_res[1]]\n","titles = [\"Negative Covariance\", \"Zero Covariance\", \"Positive Covariance\"]\n","\n","for i in range(3):\n","    ax[i].scatter(samples[i][:, 0], samples[i][:, 1], c=\"green\")\n","    ax[i].set_xlabel(\"X1\")\n","    ax[i].set_ylabel(\"X2\")\n","    ax[i].set_title(titles[i])\n","    ax[i].tick_params(labelleft=True)\n","    ax[i].axvline(x=mu[0], color=\"blue\", linestyle=\"--\")\n","    ax[i].axhline(y=mu[1], color=\"red\", linestyle=\"--\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The `blue` and `red` lines in the above plot represent the mean values of `x1` and `x2` respectively. Let's visualize how the probability density surface changes with the covariance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:55:07.417568Z","iopub.status.busy":"2022-08-03T05:55:07.417077Z","iopub.status.idle":"2022-08-03T05:55:07.446268Z","shell.execute_reply":"2022-08-03T05:55:07.445101Z","shell.execute_reply.started":"2022-08-03T05:55:07.417526Z"},"trusted":true},"outputs":[],"source":["fig = go.Figure(\n","    go.Surface(\n","        z=neg_cov_res[2],\n","        x=x1,\n","        y=x2,\n","        colorscale='Hot',\n","        showscale=False\n","    )\n",")\n","fig.update_layout(\n","    {\n","        \"title\": dict(\n","            text=f\"Bivariate Distribution<br>cov_x1x2: {neg_cov[0][1]:.2f}\",\n","            y=0.95,\n","            x=0.5,\n","            xanchor=\"center\",\n","            yanchor=\"top\",\n","            font=dict(size=12)\n","        ), \n","        \"scene\":dict(\n","            xaxis=dict(title='X1'),\n","            yaxis=dict(title='X2'),\n","            zaxis=dict(title='Probability density')\n","        ),\n","        \"xaxis\": dict(title=\"Values\"),\n","        \"yaxis\": dict(title=\"Probability Density\"),\n","    }\n",")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:55:14.952829Z","iopub.status.busy":"2022-08-03T05:55:14.951925Z","iopub.status.idle":"2022-08-03T05:55:14.979644Z","shell.execute_reply":"2022-08-03T05:55:14.978391Z","shell.execute_reply.started":"2022-08-03T05:55:14.95279Z"},"trusted":true},"outputs":[],"source":["fig = go.Figure(\n","    go.Surface(\n","        z=zero_cov_res[2],\n","        x=x1,\n","        y=x2,\n","        colorscale='Viridis',\n","        showscale=False\n","    )\n",")\n","fig.update_layout(\n","    {\n","        \"title\": dict(\n","            text=f\"Bivariate Distribution<br>cov_x1x2: {zero_cov[0][1]:.2f}\",\n","            y=0.95,\n","            x=0.5,\n","            xanchor=\"center\",\n","            yanchor=\"top\",\n","            font=dict(size=12)\n","        ), \n","        \"scene\":dict(\n","            xaxis=dict(title='X1'),\n","            yaxis=dict(title='X2'),\n","            zaxis=dict(title='Probability density')\n","        ),\n","        \"xaxis\": dict(title=\"Values\"),\n","        \"yaxis\": dict(title=\"Probability Density\"),\n","    }\n",")\n","\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:55:20.538455Z","iopub.status.busy":"2022-08-03T05:55:20.537365Z","iopub.status.idle":"2022-08-03T05:55:20.565513Z","shell.execute_reply":"2022-08-03T05:55:20.564334Z","shell.execute_reply.started":"2022-08-03T05:55:20.538394Z"},"trusted":true},"outputs":[],"source":["fig = go.Figure(\n","    go.Surface(\n","        z=pos_cov_res[2],\n","        x=x1,\n","        y=x2,\n","        showscale=False\n","    )\n",")\n","fig.update_layout(\n","    {\n","        \"title\": dict(\n","            text=f\"Bivariate Distribution<br>cov_x1x2: {pos_cov[0][1]:.2f}\",\n","            y=0.95,\n","            x=0.5,\n","            xanchor=\"center\",\n","            yanchor=\"top\",\n","            font=dict(size=12)\n","        ), \n","        \"scene\":dict(\n","            xaxis=dict(title='X1'),\n","            yaxis=dict(title='X2'),\n","            zaxis=dict(title='Probability density')\n","        ),\n","        \"xaxis\": dict(title=\"Values\"),\n","        \"yaxis\": dict(title=\"Probability Density\"),\n","    }\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["A few things to note in the above surface plots:\n","\n","1. The plot with zero covariance is circular in every direction.\n","2. The plots with negative and positive covariances are more flattened on the 45-degree line, and are somewhat in a perpendicular direction to that line visually."]},{"cell_type":"markdown","metadata":{},"source":["# 3. Isotropic Gaussian\n","\n","An isotropic Gaussian is one where the covariance matrix $\\Sigma$ can be represented in this form:\n","$$\n","\\Sigma = \\sigma^2 I  \\tag{4}\n","$$\n","\n","1. $I$ is the Identity matrix\n","2. $\\sigma^2$ is the scalar variance \n","\n","**Q**: Why do we want to represent the covarince matrix in this form?<br/>\n","**A**: As the dimensions of a multivariate Gaussian grows, the **mean** $\\mu$ follows a linear growth where the **covarince matrix** $\\Sigma$ follows quadratic growth in terms of number of parameters. This quadratic growth isn't computation friendly. A **diagonal covariance matrix** makes things much easier\n","\n","A few things to note about isotropic Gaussian:\n","\n","1. *Eq. (4)* represents a diagnal matrix multiplied by a scalar variance. This means that the **variance** along each dimesnion is equal. Hence an *isotropic multivariate Gaussian* is **circular** or **spherical**.\n","\n","2. We discussed that `Cov(x1, x2)=0` doesn't mean `x1` and `x2` are independent but if the distribution is **multivariate normal** and `Cov(x1, x2)=0`, it implies that `x1` and `x2` are independent. \n","\n","3. If the multivariate distribution is isotropic, that means:\n","       i. Covariance Matrix is diagonal\n","       ii. The distribution can be represented as a product of univariate Gaussians i.e.\n","                P(X) = P(x1)P(x2)..\n","                \n","Let's take an example of an isotropic normal"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:55:48.509227Z","iopub.status.busy":"2022-08-03T05:55:48.5081Z","iopub.status.idle":"2022-08-03T05:55:48.526173Z","shell.execute_reply":"2022-08-03T05:55:48.524995Z","shell.execute_reply.started":"2022-08-03T05:55:48.509168Z"},"trusted":true},"outputs":[],"source":["# Mean of the random variables X1 and X2\n","mu_x1, mu_x2 = 0, 0\n","\n","# Standard deviation of the random variables X1 and X2\n","# Remember the std. is going to be the same\n","# along all the dimesnions.\n","sigma_x1 = sigma_x2 = 2\n","\n","# Number of samples to extract\n","sample_size = 5000\n","\n","# Positions for which probability density is\n","# to be generated\n","x1, x2 = np.mgrid[\n","    (-3.0 * sigma_x1 + mu_x1):(3.0 * sigma_x1 + mu_x1): 0.1,\n","    (-3.0 * sigma_x2 + mu_x2):(3.0 * sigma_x2 + mu_x2): 0.1\n","]\n","pos = np.dstack((x1, x2))\n","\n","# Mean vector\n","mu = [mu_x1, mu_x2]\n","\n","\n","# Because the covariance matrix of an multivaraite isotropic\n","# gaussian is a diagonal matrix, hence the covariance for \n","# the dimesnions will be zero.\n","cov_x1x2 = 0\n","cov = [[sigma_x1**2, cov_x1x2], [cov_x1x2, sigma_x2**2]]\n","\n","\n","isotropic_gaussian = get_multivariate_normal(\n","                        mu=mu,\n","                        cov=cov,\n","                        sample=True,\n","                        sample_size=sample_size,\n","                        seed=seed,\n","                        gen_pdf=True,\n","                        pos=pos\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:55:49.157784Z","iopub.status.busy":"2022-08-03T05:55:49.156558Z","iopub.status.idle":"2022-08-03T05:55:49.363517Z","shell.execute_reply":"2022-08-03T05:55:49.362232Z","shell.execute_reply.started":"2022-08-03T05:55:49.157725Z"},"trusted":true},"outputs":[],"source":["fig = make_subplots(\n","    rows=1, cols=2,\n","    shared_yaxes=False,\n","    shared_xaxes=False,\n","    specs=[[{'type': 'scatter'}, {'type': 'surface'}]],\n","    subplot_titles=(\n","        \"Covariance x1_x2 = 0.0\",\n","        f\"mu_x1: {mu_x1} sigma_x1: {sigma_x1**2} <br>mu_x2: {mu_x2} sigma_x2: {sigma_x2**2}\"\n","    )\n",")\n","\n","\n","fig.add_trace(\n","    go.Scatter(\n","        x=isotropic_gaussian[1][:, 0],\n","        y=isotropic_gaussian[1][:, 1],\n","        mode='markers',\n","        marker=dict(size=5, color=\"green\"),\n","    ),\n","    row=1, col=1\n",")\n","\n","fig.add_trace(\n","    go.Surface(\n","        z=isotropic_gaussian[2],\n","        x=x1,\n","        y=x2,\n","        colorscale='RdBu',\n","        showscale=False\n","    ),\n","    row=1, col=2\n",")\n","\n","fig.update_layout(\n","    {\n","        \"scene\":dict(\n","            xaxis=dict(title='X1'),\n","            yaxis=dict(title='X2'),\n","            zaxis=dict(title='Probability density')\n","        ),\n","        \"xaxis\": {\"title\": \"X1\"},\n","        \"yaxis\": {\"title\": \"X2\"},\n","        \"title\": {\"text\": \"Isotropic Gaussian\", \"x\":0.5, \"font\":dict(size=20)}\n","    }\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Conditional Distribution \n","\n","Let's say we have a multivariate distribution over `x` where\n","$$\n"," x = \\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix} \\\\\n"," \\mu = \\begin{bmatrix}\\mu_{1}\\\\\\mu_{2}\\end{bmatrix} \\\\\n"," \\Sigma = \\begin{bmatrix}\\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22}\\end{bmatrix}\n","$$\n","\n","Then the distribution of `x1` conditional on `x2=a` is multivariate normal:\n","\n","$$\n","p(x1 | x2 = a) \\sim N(\\bar\\mu, \\bar\\Sigma) \\ \\ \\ \\text  {where:}\n","$$\n","\n","$$\n","\\bar\\mu = \\mu_{1} + \\ \\Sigma_{12}\\Sigma_{22}^{-1}(a - \\mu_{2}) \\tag{5}\n","$$\n","$$\n","\\bar\\Sigma = \\Sigma_{11} - \\ \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} \\tag{6}\n","$$\n","\n","<br/>\n","<br/>\n","\n","But why are we discussing the conditional distribution? Do you need to remember these equations?\n","\n","Well, first and foremost you don't have to mug up any of these equations. You can find them easily on Wikipedia. But there is a reason why we are discussing the conditional distributions here.\n","\n","Let's say you have a process that takes the form of a [**Markov Chain**](https://setosa.io/ev/markov-chains/). For example, the forward process in **Diffusion Models** is one example of such a sequence. Let's write down the equation for the same.\n","$$\n","q(x_{1:T}\\vert x_{0}) := \\prod_{t=1}^{T}q(x_{t}\\vert x_{t-1}) :=\\prod_{t=1}^{T}\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}} x_{t-1},\\ \\beta_{t}\\bf I) \\tag{7}\n","$$\n","\n","This equation tells us a few things:\n","\n","1. The forward process is a Markov Chain where the sample at the present step depends only on the sample at the previous timestep.\n","2. The covariance matrix is **diagonal**\n","3. At each timestep in this sequence, we gradually **add** Gaussian noise. But this isn't very clear from the equation where is this addition taking place, right?\n","4. The term $\\beta$ represents the variance at a *particular timestep `t`* such that $0 < \\beta_{1} < \\beta_{2} < \\beta_{3} < ...  < \\beta_{T} < 1$\n","\n","To understand the last point, let's make some assumptions to simplify the eq (7).<br/>\n","1. We will ignore the variance schedule $\\beta$ for now. We can rewrite the above equation as: $$q(x_{1:T}\\vert x_{0}) := \\prod_{t=1}^{T}q(x_{t}\\vert x_{t-1}) :=\\prod_{t=1}^{T}\\mathcal{N}(x_{t}; x_{t-1},\\ \\bf I)  \\tag{8}$$\n","2. We will consider `x` as a univariate normal. This assumption is made only to give readers a better understanding of the conditional Gaussian case. The same concept extends to multivariate normal.\n","\n","With the above assumptions in mind, combined with the `Law of total probability`, we can write the equation as:\n","\n","$$p(x_{t}) = \\int p(x_{t} \\vert \\ x_{t-1})p(x_{t-1})dx_{t-1} \\tag {9}$$\n","\n","Using *(8)*, we can rewrite this as:\n","$$p(x_{t}) = \\int \\mathcal{N}(x_{t}; \\ x_{t-1}, 1) p(x_{t-1})dx_{t-1} \\tag {10}$$\n","\n","Because we are dealing with univariate normal, the identity matrix is nothing but a scalar value of 1. Moving forward, we can shift the terms in the above equation like this:\n","$$p(x_{t}) = \\int \\mathcal{N}(x_{t} - x_{t-1}; \\ 0, 1) p(x_{t-1})dx_{t-1} \\tag {11}$$\n","\n","Notice two things that we got by this shift:\n","1. The mean and the variance values of the distribution on the right-hand side are `0` and `1` respectively.\n","2. If you look closely at the term on the RHS in the above equation, you will notice that it is the definition of **convolution**.\n","\n","Combining the above two facts, we can rewrite *(11)* as:\n","$$p(x_{t}) = \\mathcal{N}(0, 1) \\ * \\ p(x_{t-1}) \\tag {12}$$\n","\n","**Property**: The convolution of individual distributions of two or more random variables equals the sum of the random variables.\n","\n","$$ \\Longrightarrow x_{t} = \\mathcal{N}(0, 1) \\ + \\ x_{t-1} \\tag {13}$$\n","\n","We will prove the above property in the next section, but this is one of the things that you should remember. Also, we hope that it is clear now why conditioned distributions in the forward process of Diffusion models are equivalent to \"adding Gaussian noise\" to previous timesteps. Don't worry about the diffusion models related equations and terms like `variance schedule`. We will talk about them in detail in the upcoming notebooks."]},{"cell_type":"markdown","metadata":{},"source":["# 5. Convolution of probability distributions\n","\n","In the last section we stated that the convolution of individual distributions of two or more random variables equals the sum of the random variables. To prove this, we will take an example of two normally distributed random variables `X` and `Y`. If `X` and `Y` are two normally distributed independent random variables such that\n","\n","$$\n","X \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^2) \\\\\n","Y \\sim \\mathcal{N}(\\mu_{2}, \\sigma_{2}^2) \\\\\n","$$\n","\n","$\\text{if} \\ Z = X + Y  \\Longrightarrow Z \\sim \\mathcal{N}(\\mu_{1} + \\mu_{2}, \\sigma_{1}^2 +\\sigma_{2}^2) \\tag{14}$ \n","\n","Our goal is to prove that if we convolve the PDFs of `X` and `Y`, then the resulting distribution would be identical to the distribution of `Z`. Let's write down the code for it."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:57:11.448846Z","iopub.status.busy":"2022-08-03T05:57:11.447703Z","iopub.status.idle":"2022-08-03T05:58:27.323366Z","shell.execute_reply":"2022-08-03T05:58:27.322058Z","shell.execute_reply.started":"2022-08-03T05:57:11.448796Z"},"trusted":true},"outputs":[],"source":["# Mean and Standard deviation of X\n","mu_x = 0.0\n","sigma_x = 2.0\n","\n","# Mean and Standard deviation of Y\n","mu_y = 2.0\n","sigma_y = 4.0\n","\n","# Mean and Standard deviation of Z\n","mu_z = mu_x + mu_y\n","sigma_z = np.sqrt(sigma_x**2 + sigma_y**2)\n","\n","# Get the distributions\n","dist_x = norm(loc=mu_x, scale=sigma_x)\n","dist_y = norm(loc=mu_y, scale=sigma_y)\n","dist_z = norm(loc=mu_z, scale=sigma_z)\n","\n","# Generate the PDFs\n","step_size = 1e-4\n","points = np.arange(-30, 30, step_size)\n","\n","pdf_x = dist_x.pdf(points)\n","pdf_y = dist_y.pdf(points)\n","pdf_z = dist_z.pdf(points)\n","\n","\n","# NOTE: We cannot convolve over continous functions using `numpy.convolve(...)`\n","# Hence we will discretize our PDFs into PMFs using the step size we defined above\n","pmf_x = pdf_x * step_size\n","pmf_y = pdf_y * step_size\n","\n","# Convolve the two PMFs\n","conv_pmf = np.convolve(pmf_x, pmf_y, mode=\"same\")\n","conv_pdf = conv_pmf / step_size\n","\n","\n","# Let's plot the distributions now and check if we have gotten\n","# the same distribution as Z. \n","# NOTE: As we have approximated PMF from PDF, there would be\n","# erros in the approximation. So, the final result may not\n","# look 100% identical.\n","\n","_, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(12, 5))\n","\n","ax[0].plot(points, pdf_x)\n","ax[0].plot(points, pdf_y)\n","ax[0].plot(points, pdf_z)\n","ax[0].set_title(\"Z = X + Y\")\n","ax[0].legend([\"PDF-X\", \"PDF-Y\", \"PDF-Z\"])\n","\n","ax[1].plot(points, pdf_x)\n","ax[1].plot(points, pdf_y)\n","ax[1].plot(points, pdf_z)\n","ax[1].set_title(\"Convolution of PDFs of X and Y\")\n","ax[1].legend([\"PDF-X\", \"PDF-Y\", \"Convolved\"])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 6. The Forward Process\n","\n","Though we will talk about `Diffusion Models` in detail in the future posts as well, I will implement the forward process here to give you an idea that things that might look complicated in symbols aren't that complicated in terms of code.\n","\n","Let's rewrite the forward process equation again:\n","\n","$$\n","q(x_{1:T}\\vert x_{0}) := \\prod_{t=1}^{T}q(x_{t}\\vert x_{t-1}) :=\\prod_{t=1}^{T}\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}} x_{t-1},\\ \\beta_{t}\\bf I)\n","$$\n","\n","To implement the above equation:\n","\n","1. We have to define the total number of timesteps `T`\n","2. We have to generate $\\beta_{t}$ using a schedule. We can use any schedule including but not limited to linear, quadratic, etc. The only thing that we need to ensure is that $\\beta_{1} < \\beta_{2}...$\n","3. Sample a new image at timestep `t` from a conditional Gaussian for which the paramters are $\\mu_{t} = \\sqrt{1-\\beta_{t}} x_{t-1}$ and $\\sigma_{t}^2 = \\beta_{t}$\n","4. For the last point, we can use the property(eq. (13)) we studied in the conditional distribution section. Hence we can write this as:\n","\n","$$x_{t} \\sim (\\mathcal{N}(\\sqrt{1-\\beta_{t}} x_{t-1},\\ \\beta_{t}) + \\mathcal{N}(0, 1))$$\n","\n","$$\n","\\Rightarrow x_{t} = \\sqrt{1-\\beta_{t}} x_{t-1} + \\sqrt{\\beta_{t}}\\epsilon \\ \\ ; \\ \\text{where} \\ \\epsilon \\sim \\mathcal{N}(0, 1) \\tag{15}\n","$$\n","\n","Now that we have broken down the equation in much simpler parts, let's code it!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T05:58:27.326207Z","iopub.status.busy":"2022-08-03T05:58:27.32569Z","iopub.status.idle":"2022-08-03T05:58:28.241628Z","shell.execute_reply":"2022-08-03T05:58:28.240349Z","shell.execute_reply.started":"2022-08-03T05:58:27.326159Z"},"trusted":true},"outputs":[],"source":["def forward_process_ddpms(img_t_minus_1, beta, t):\n","    \"\"\"Implements the forward process of a DDPM model.\n","    \n","    Args:\n","        img_t_minus_1: Image at the previous timestep (t - 1)\n","        beta: Scheduled Variance\n","        t: Current timestep\n","    Returns:\n","        Image obtained at current timestep\n","    \"\"\"\n","    \n","    # 1. Obtain beta_t. Reshape it to have the same number of\n","    # dimensions as our image array\n","    beta_t = beta[t].reshape(-1, 1, 1)\n","    \n","    # 2. Calculate mean and variance\n","    mu = np.sqrt((1.0 - beta_t)) * img_t_minus_1\n","    sigma = np.sqrt(beta_t)\n","    \n","    # 3. Obtain image at timestep t using equation (15)\n","    img_t = mu + sigma * np.random.randn(*img_t_minus_1.shape)\n","    return img_t\n","\n","\n","# Let's download a sample image and check if our\n","# forward process function is doing what it is supposed to do\n","urllib.request.urlretrieve(\n","    \"https://upload.wikimedia.org/wikipedia/commons/1/18/Dog_Breeds.jpg\",\n","    \"dog.jpg\"\n",")\n","\n","# 1. Load image using PIL (or any other library that you prefer)\n","img = Image.open(\"dog.jpg\")\n","\n","# 2. Resize the image to desired dimensions\n","IMG_SIZE = (128, 128)\n","img = img.resize(size=IMG_SIZE)\n","\n","# 3. Define number of timesteps\n","timesteps = 100\n","\n","# 4. Generate beta (variance schedule)\n","beta_start = 0.0001\n","beta_end = 0.05\n","beta = np.linspace(beta_start, beta_end, num=timesteps, dtype=np.float32)\n","\n","\n","processed_images = []\n","img_t = np.asarray(img.copy(), dtype=np.float32) / 255.\n","\n","# 5. Run the forward process to obtain img after t timesteps\n","for t in range(timesteps):\n","    img_t = forward_process_ddpms(img_t_minus_1=img_t, beta=beta, t=t)\n","    if t%20==0 or t==timesteps - 1:\n","        sample = (img_t.clip(0, 1) * 255.0).astype(np.uint8)\n","        processed_images.append(sample)\n","\n","# 6. Plot and see samples at different timesteps\n","_, ax = plt.subplots(1, len(processed_images), figsize=(12, 5))\n","\n","for i, sample in enumerate(processed_images):\n","    ax[i].imshow(sample)\n","    ax[i].set_title(f\"Timestep: {i*20}\")\n","    ax[i].axis(\"off\")\n","    ax[i].grid(False)\n","\n","plt.suptitle(\"Forward process in DDPMs\", y=0.75)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["One thing to note here is that as you increase the number of timesteps `T`, $\\beta_{t} \\to 1$. At that point:\n","\n","$$q(x_{T}) \\approx \\mathcal{N}(x_{T};\\ 0, I)$$\n","\n","\n","That's it for now. We will talk about reverse process and other things related to DDPMs in the future notebooks. I hope this notebook was enough to give you a solid understanding of **Gaussian distribution** and their usage in context of DDPMs. Also, if you liked this work, please show your support by giving an upvote!\n","\n","\n","# References:\n","\n","* https://en.wikipedia.org/wiki/Normal_distribution\n","* https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Likelihood_function\n","* https://en.wikipedia.org/wiki/Convolution_of_probability_distributions\n","* https://online.stat.psu.edu/stat505/lesson/6/6.1\n","* https://www.inf.ed.ac.uk/teaching/courses/pmr/slides/cts4up.pdf\n","* http://timstaley.co.uk/posts/convolving-pdfs-in-python/\n","* https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/\n","* https://www.youtube.com/watch?v=fbLgFrlTnGU&t=317s\n","* https://people.eecs.berkeley.edu/~jrs/189s17/lec/09.pdf\n","* https://mmuratarat.github.io/2019-10-05/univariate-multivariate_gaussian\n","* https://math.stackexchange.com/questions/4492913/isotropic-gaussian\n","* https://cs229.stanford.edu/section/gaussians.pdf\n","* https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic\n","* https://www.cse.psu.edu/~rtc12/CSE586Spring2010/papers/prmlGaussian.pdf\n","* https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html\n","* https://docs.scipy.org/doc/scipy/tutorial/stats.html\n","* https://www.deeplearningbook.org/contents/prob.html\n","* https://arxiv.org/abs/2006.11239"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import scipy\n","import urllib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","seed = 1234\n","np.random.seed(seed)\n","plt.style.use(\"seaborn\")\n","\n","%config IPCompleter.use_jedi = False\n","%config InlineBackend.figure_format='png'\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["# 1. The Forward Process\n","\n","$$\n","q(x_{1:T}\\vert x_{0})\n",":= \\prod_{t=1}^{T}q(x_{t}\\vert x_{t-1})\n",":=\\prod_{t=1}^{T}\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}} x_{t-1},\\ \\beta_{t}\\bf I) \\tag{1}\n","$$\n","\n","As a refresher, $q(x_{0:T})$ is known as the **forward distribution** and $q(x_{t}\\vert x_{t-1}$ is referred as **forward diffusion kernel**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def forward_process_ddpms(img_t_minus_1, beta, t):\n","    \"\"\"Implements the forward process of a DDPM model.\n","    \n","    Args:\n","        img_t_minus_1: Image at the previous timestep (t - 1)\n","        beta: Scheduled Variance\n","        t: Current timestep\n","    Returns:\n","        Image obtained at current timestep\n","    \"\"\"\n","    \n","    # 1. Obtain beta_t. Reshape it to have the same number of\n","    # dimensions as our image array\n","    beta_t = beta[t].reshape(-1, 1, 1)\n","    \n","    # 2. Calculate mean and variance\n","    mu = np.sqrt((1.0 - beta_t)) * img_t_minus_1\n","    sigma = np.sqrt(beta_t)\n","    \n","    # 3. Obtain image at timestep t using equation (15)\n","    img_t = mu + sigma * np.random.randn(*img_t_minus_1.shape)\n","    return img_t\n","\n","\n","# Let's check if ourforward process function is\n","# doing what it is supposed to do on a sample image\n","urllib.request.urlretrieve(\n","    \"https://upload.wikimedia.org/wikipedia/commons/1/18/Dog_Breeds.jpg\",\n","    \"dog.jpg\"\n",")\n","\n","# 1. Load image using PIL (or any other library that you prefer)\n","img = Image.open(\"dog.jpg\")\n","\n","# 2. Resize the image to desired dimensions\n","IMG_SIZE = (128, 128)\n","img = img.resize(size=IMG_SIZE)\n","\n","# 3. Define number of timesteps\n","timesteps = 100\n","\n","# 4. Generate beta (variance schedule)\n","beta_start = 0.0001\n","beta_end = 0.05\n","beta = np.linspace(beta_start, beta_end, num=timesteps, dtype=np.float32)\n","\n","\n","processed_images = []\n","img_t = np.asarray(img.copy(), dtype=np.float32) / 255.\n","\n","# 5. Run the forward process to obtain img after t timesteps\n","for t in range(timesteps):\n","    img_t = forward_process_ddpms(img_t_minus_1=img_t, beta=beta, t=t)\n","    if t%20==0 or t==timesteps - 1:\n","        sample = (img_t.clip(0, 1) * 255.0).astype(np.uint8)\n","        processed_images.append(sample)\n","\n","# 6. Plot and see samples at different timesteps\n","_, ax = plt.subplots(1, len(processed_images), figsize=(15, 6))\n","\n","for i, sample in enumerate(processed_images):\n","    ax[i].imshow(sample)\n","    ax[i].set_title(f\"Timestep: {i*20}\")\n","    ax[i].axis(\"off\")\n","    ax[i].grid(False)\n","\n","plt.suptitle(\"Forward process in DDPMs\", y=0.75)\n","plt.show()\n","plt.close()"]},{"cell_type":"markdown","metadata":{},"source":["Can you spot a major problem with the above equation? (Hint: Check the loop)\n","\n","Don't worry if you didn't get it. Look at the above code closely. Forget everything from the modelling perspective except for the forward pass. You will notice that to obtain a noisy sample, say at timestep `t`, we need to iterate from `t0` to `t-1`. Why? Because the sample obtained at each timestep is conditioned on the samples from the previous timesteps. \n","\n","That's not efficient. What if there are 1000 steps and you want to sample the `999th` timestep? You will iterate the whole loop, simulating the entire Markov Chain. Now that we know the problem, we should think about how we can do better.\n","\n","\n","\n","# 2. Reparameterization\n","\n","We know that sum of the independent Gaussians is still a Gaussian. We can leverage this fact to sample from an arbitrary forward step. All we need to do is apply the `reparameterization` trick.\n","\n","Let &nbsp; $\\alpha_{t} = 1 - \\beta_{t},$ &nbsp; and  &nbsp; $\\bar{\\alpha}_{t} = \\prod_{i=1}^T \\alpha_{i}$\n","\n","From equation (1) we know that:\n","\n","\n","$$\n","q(x_{1:T}\\vert x_{0}) := \\prod_{t=1}^{T}q(x_{t}\\vert x_{t-1}) :=\\prod_{t=1}^{T}\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}} x_{t-1},\\ \\beta_{t} \\bf{I}) \\\\\n","$$\n","\n","$$\n","\\text{or} \\ \\ q(x_{t}\\vert x_{t-1}) = \\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}} x_{t-1},\\ \\beta_{t} \\bf{I})\n","$$\n","\n","We can obtain the sample at timestep `t` as:\n","\n","$$\n","x_{t} = \\sqrt{1 - \\beta_{t}} x_{t-1} +  \\sqrt{\\beta_{t}}\\epsilon_{t-1}; \\ \\ \\text where \\ \\  \\epsilon_{t-1} \\sim \\mathcal{N}(0, \\bf{I})\n","$$\n","\n","Replacing $\\beta$ with $\\alpha$ in the above equation we can obtain sample at timestep `t` as:\n","\n","$$\n","\\begin{align*}\n","x_{t} &= \\sqrt{\\alpha_{t}} x_{t-1} + \\sqrt{1 - \\alpha_{t}}\\epsilon_{t-1} \\\\\n","\\Rightarrow x_{t} &= \n","\\sqrt{\\alpha_{t}} \\ \\ \\underbrace{(\\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}}\\epsilon_{t-2})}_{\\text{( Expanding } x_{t-1})} + \n","\\sqrt{1 - \\alpha_{t}}\\epsilon_{t-1} \\\\ \\\\\n","&= \\sqrt{\\alpha_{t} \\alpha_{t-1}} x_{t-2} + \n","\\underbrace{\\sqrt{\\alpha_{t}(1 - \\alpha_{t-1})}\\epsilon_{t-2}}_{\\text{RV1}} + \n","\\underbrace{\\sqrt{1 - \\alpha_{t}}\\epsilon_{t-1}}_{\\text{RV2}} \\\\\n","\\end{align*}\n","$$\n","\n","The two terms namely RV1, and RV2 on RHS in the above equation are two random variables distributed **normally** with a mean of zero and variances $\\alpha_{t}(1 - \\alpha_{t-1})\\ $, and $(1 - \\alpha_{t})$ respectively. \n","\n","In the last [lesson](https://www.kaggle.com/code/aakashnain/ddpms-part-1) we learned that if we have two Gaussian distributions with mean values $\\mu_{1} , \\mu_{2}$ and variances $\\sigma_{1}^2 , \\sigma_{2}^2$ respectively, then the sum of these two random variables is equivaluent to another random variable with a normal distribution $\\mathcal{N}(\\mu_{1} + \\mu_{2}, \\sigma_{1}^2 +\\sigma_{2}^2)$. Applying this to the above equation yields:\n","\n","$$\n","\\begin{align*}\n","\\Rightarrow x_{t} &= \n","\\sqrt{\\alpha_{t} \\alpha_{t-1}} x_{t-2} + \n","\\sqrt{\\alpha_{t}(1 - \\alpha_{t-1}) + \n","(1 - \\alpha_{t})}\\bar{z}_{t-2} & \\bar{z}_{t-2} \\ \\text {is the merged Gaussian} \\\\\n","&= \\sqrt{\\alpha_{t} \\alpha_{t-1}} x_{t-2} + \n","\\sqrt{1 - \\alpha_{t} \\alpha_{t-1}}\\bar{z}_{t-2} \\\\\n","&= \\ \\ ... \\\\\n","\\Rightarrow x_{t}&= \\sqrt{\\bar{\\alpha_{t}}} x_{0} + \n","\\sqrt{1 - \\bar{\\alpha_{t}}}\\epsilon \\ \\  \\ \\text{ (since } \\\n","\\ \\bar{\\alpha}_{t} = \\prod_{i=1}^T \\alpha_{i})\n","\\end{align*}\n","$$\n","\n","From above, we can say that:\n","\n","$$\n","q(x_{t}\\vert x_{0}) = \\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha_{t}}} x_{0},\\ (1 - \\bar{\\alpha_{t}}) \\ \\bf{I}) \\tag {2}\n","$$\n","\n","Ha! The above equation is nice.Given the original image, we can now sample at any arbitrary timestep without simulating the entire Markov chain till that step. Before coding it up, let's recap of what we did and achieve:\n","\n","1. We figured out that in the original formulation, we need to simulate the Markov chain to the step for which we want to sample.\n","2. We reparameterized $\\beta$ in terms of $\\alpha$\n","3. The above reparameterization leads to an equation where we can sample at any arbitrary timestep \n","\n","\n","Let's code it and compare the results with the previous results "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def forward_process_ddpms_v2(orig_img, alpha_bar, t):\n","    \"\"\"Implements the efficient forward process of a DDPM model.\n","    \n","    Args:\n","        orig_img: Image at timestep t=0\n","        alpha_bar: The reparameterized version of beta\n","        t: Current timestep\n","    Returns:\n","        Image obtained at current timestep\n","    \"\"\"\n","    \n","    # 1. Obtain beta_t. Reshape it to have the same number of\n","    # dimensions as our image array\n","    alpha_bar_t = alpha_bar[t].reshape(-1, 1, 1)\n","    \n","    # 2. Calculate mean and variance\n","    mu = np.sqrt(alpha_bar_t) * orig_img\n","    sigma = np.sqrt(1.0 - alpha_bar_t)\n","    \n","    # 3. Obtain image at timestep t\n","    img_t = mu + sigma * np.random.randn(*orig_img.shape)\n","    return img_t\n","\n","\n","\n","# 1. Define alpha and alpha_bar\n","alpha = 1.0 - beta\n","alpha_bar = np.cumprod(alpha)\n","\n","\n","processed_images = [img] # Image at 0th step\n","orig_img = np.asarray(img.copy(), dtype=np.float32) / 255.\n","\n","\n","# 2. Run the forward pass for specific timesteps\n","# We will use the timesteps we used in previous visualizations\n","specific_timesteps = [19, 39, 59, 79, 99]\n","for step in specific_timesteps:\n","    img_t = forward_process_ddpms_v2(orig_img, alpha_bar, step)\n","    img_t = (img_t.clip(0, 1) * 255.0).astype(np.uint8)\n","    processed_images.append(img_t)\n","\n","    \n","# 3. Plot and see samples at different timesteps\n","_, ax = plt.subplots(1, len(processed_images), figsize=(15, 6))\n","\n","for i, sample in enumerate(processed_images):\n","    ax[i].imshow(sample)\n","    ax[i].set_title(f\"Timestep: {i*20}\")\n","    ax[i].axis(\"off\")\n","    ax[i].grid(False)\n","\n","plt.suptitle(\"Efficient Forward process in DDPMs\", y=0.75)\n","plt.show()\n","plt.close()"]},{"cell_type":"markdown","metadata":{},"source":["Now that we are aware of everything involved in the forward process, we need to figure out how are we going to generate image from the noise we got at the end of the forward process. \n","\n","\n","# 2. The Reverse Process\n","\n","We should end up with a pure noise distribution by the end of the forward process, given we set the variance schedule appropriately i.e. the distribution we will end up with will be $\\sim \\mathcal{N}(x_{T}; 0,I)$. For the reverse process, we will start with noise, and will try to undo the noise at each timestep to obtain back the original image. We can write this process as:\n","\n","$$\n","p_{\\theta}(x_{0:T}) \n",":= p(x_{T}) \\prod_{t=1}^T p_{\\theta}(x_{t-1} | x_{t}) \n",":= p(x_{T}) \\prod_{t=1}^T \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t}, t)) \\tag{3}\n","$$\n","\n","where the parameters of the multivariate Gaussian are time-dependent and are to be learned.\n","\n","A few things to note:\n","\n","1. $p(x_{0:T})$ is the **reverse distribution** and $p(x_{t-1} | x_{t})$ is known as the **reverse diffusion kernel**\n","2. $\\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t}$ are the learnable parameters of the reverse distribution\n","3. The forward process can be seen as pushing the sample off the data mainfold, turning it into noise. The reverse process can be seen as pushing the sample back to the manifold by removing the noise. (Words taken from Ari Seff's [tutorial](https://www.youtube.com/watch?v=fbLgFrlTnGU) because I don't think there is any better way to put it.)\n","4. $p(x_{T})$ is nothing but $q(x_{T})$ i.e. the point where the forward process ends is the starting point of the reverse process.\n","5. There can be `n` number of pathways to arrive at a sample $p_{\\theta}(x_{0})$ starting from a noise sample. To obtain $p_{\\theta}(x_{0})$, we would then be required to integrate over all the possible pathways i.e. $p_{\\theta}(x_{0}) = \\int p_{\\theta}(x_{0:T})dx_{1:T}$\n","6. Calculating density like in the above step is intractable. A neural network is sufficient to predict the mean $\\mu_{\\theta}$ and the diagonal covariance matrix $\\Sigma_{\\theta}$ for the reverse process as shown below in the equation, but we would also be required to frame our objectve function differently\n","\n","\n","\n","# 3. The Training Objective\n","\n","Let's write down all the equations we saw for the reverse process, again before we move to the discussion on training objective. \n","\n","$$\n","\\begin{align*}\n","p_{\\theta}(x_{0}) &= \\int p_{\\theta}(x_{0:T})dx_{1:T} \\tag{4} \\\\\n","p_{\\theta}(x_{t-1}|x_{t}) &:= \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t}, t)) \\tag{5}\n","\\end{align*}\n","$$\n","\n","If eq.*(4)* is intractable, then how do we frame our objective function? \n","\n","If you have worked with other types of generative models before, then you might have seen something like equation eq.*(4)* in other types of generative models. But where? The answer is **Variational AutoEncoder**(VAE for short). If we treat $x_{0}$ as **observed variable** and $x_{1:T}$ as **latent variables**, then we have a setup similar to a VAE.\n","\n","Similarities:\n","1. The forward process can be seen as the equivalent of the encoder in a VAE converting data to latents\n","2. The reverse process can be seen as the equivalent of the decoder in a VAE producing data from latents\n","3. Given that the above two hold, we can maximize the lower bound (Check out this [post](https://lilianweng.github.io/posts/2018-08-12-vae/#loss-function-elbo) if you need a referesher on ELBO)\n","\n","Differences:\n","1. Unlike a VAE where the encoder is jointly trained with the decoder, the forward process in DDPMs is fixed. Only the reverse part is trainable\n","2. Unlike in VAEs, the latents in DDPMs have the same dimensionality as the observed variable\n","3. Variation lower bound or Evidence lower bound (ELBO for short), in the case of DDPMs, is a sum of losses at each time step `t`, $L = L_{0} + L_{1} + ... + L_{T}$ <br/> "]},{"cell_type":"markdown","metadata":{},"source":["First, let's write down the lower bound we see in a VAE here. Let's say **x** is an observed variable, and **z** is the latent variable (in context of a VAE). It is known that:\n","\n","$$\n","\\begin{align*}\n","&\\text{log}\\ p_{\\theta}(x) \\ge \\text{variational lower bound} \\\\\n","\\Rightarrow &\\text{log}\\ p_{\\theta}(x) \\ge \\mathbb{E}_{q_{\\phi}(z|x)}[\\text{log}\\ p_{\\theta}(x|z)] \\ - \\ D_{KL}(q_{\\phi}(z|x) \\parallel\tp_{\\theta}(z)) \\tag{6}\n","\\end{align*}\n","$$\n","\n","\n","As discussed earlier, when it comes to diffusion models, we can treat $x_{0}$ as the observed variable, and $x_{1:T}$ as the latent varaibles. Substituting these in equation *(6)* yields:\n","\n","$$\n","\\begin{align*}\n","\\text{log}\\ p_{\\theta}(x_{0}) &\\ge \\mathbb{E}_{q(x_{1:T}|x_{0})}[\\text{log}\\ p_{\\theta}(x_{0}|x_{1:T})] \\ - \\ \\underbrace{D_{KL}(q(x_{1:T}|x_{0}) \\parallel p_{\\theta}(x_{1:T}))} \\\\ \\\\ \n","&=\\mathbb{E}_{q(x_{1:T}|x_{0})}[\\text{log}\\ p_{\\theta}(x_{0}|x_{1:T})] \\ - \\ \\mathbb{E}_{q(x_{1:T}|x_{0})}\\big[\\text{log}\\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{1:T})}\\big] \\\\\n","&=\\mathbb{E}_{q}[\\text{log}\\ p_{\\theta}(x_{0}|x_{1:T})] \\ - \\ \n","\\mathbb{E}_{q} \\big[ \\text{log}\\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{1:T})} \\big]\n","\\\\\n","&=\\mathbb{E}_{q}\\bigg[\\text{log}\\ p_{\\theta}(x_{0}|x_{1:T}) \\ - \\ \n","\\text{log}\\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{1:T})}\\bigg]\n","\\\\\n","&=\\mathbb{E}_{q}\\biggl[\\text{log}\\ p_{\\theta}(x_{0}|x_{1:T}) \\ + \\ \\text{log}\\frac{p_{\\theta}(x_{1:T})}{q(x_{1:T}|x_{0})}\\biggr]\n","\\\\\n","&=\\mathbb{E}_{q}\\biggl[\\text{log}\\ \\frac{p_{\\theta}(x_{0}|x_{1:T}) \\ p_{\\theta}(x_{1:T})}{{q(x_{1:T}|x_{0})}}\\biggr]\n","\\\\\n","&=\\mathbb{E}_{q}\\biggl[\\text{log}\\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_{0})}\\biggr] \\tag{7} \\\\\n","\\end{align*}\n","$$\n","\n","\n","The maths above looks scary but if you look closely, we haven't done anything fancy apart from applying standard definitions of expectation, KL-Divergence, and log to the original equation. Let's denote equation *(7)* by ${L}$\n","\n","$$\n","\\begin{align*}\n","{L} &=\\mathbb{E}_{q}\\biggl[\\text{log}\\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_{0})}\\biggr] \\\\\n","&= \\mathbb{E}_{q}\\biggl[\\text{log}\\frac{p(x_{T})\\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_{t})}{\\prod_{t=1}^T q(x_{t}|x_{t-1})}\\biggr] \\\\\n","\\Rightarrow{L} &= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\underbrace{\\sum\\limits_{t=1}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})}}\\biggr]\n","\\end{align*}\n","$$\n","\n","\n","Using Bayes' rule, we know that,\n","$$\n","\\begin{align*}\n","q(x_{t-1} | x_{t}, x_{0}) &= q(x_{t} | x_{t-1}, x_{0}) \\frac{q(x_{t-1}| x_{0})}{q(x_{t} | x_{0})} \\tag{8}\n","\\end{align*}\n","$$\n","\n","$$\n","\\begin{align*}\n","{L} &= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\underbrace{\\sum\\limits_{t=1}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})}}\\biggr] \\\\\n","\\text{Using (8) we can rewrite this as:} \\\\\n","L &= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})} \\ + \\ \\text{log}\\frac{p_{\\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}\\biggr] \\\\\n","\\Rightarrow L &= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\text{log}\\frac{p_{\\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})} \\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}\\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\\\\n","\\Rightarrow L &= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\text{log}\\frac{p_{\\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})} \\ + \\ \\underbrace{\\sum\\limits_{t=2}^{T}\\text{log}\\bigl(q(x_{t-1}|x_{0})\\bigr) \\ - \\ \\text{log}\\bigl({q(x_{t}|x_{0})\\bigr)}} \\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\\\\n","\\text{Expanding that summation, we get:} \\\\\n","L &= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\text{log}\\frac{p_{\\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})} \\ + \\ \\text{log}\\frac{q(x_{1} | x_{0})}{q(x_{T}|x_{0})} \\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\\\\n","&= \\mathbb{E}_{q}\\biggl[\\text{log}\\ p(x_{T}) \\ + \\ \\text{log }{p_{\\theta}(x_{0}|x_{1})\\ - \\ \\text{log }q(x_{1}|x_{0})} \\ + \\ \\text{log }q(x_{1} | x_{0}) \\ - \\ \\text{log }{q(x_{T}|x_{0})} \\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\\\\n","&= \\mathbb{E}_{q(x_{1:T}|x_{0})}\\biggl[\\text{log} \\frac{p(x_{T})}{q(x_{T}|x_{0})} \\ + \\ \\text{log }{p_{\\theta}(x_{0}|x_{1})} \\ + \\ \\sum\\limits_{t=2}^{T}\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\\\\n","\\Rightarrow L &= \\mathbb{E}_{q(x_{1}|x_{0})}\\big[\\text{log }{p_{\\theta}(x_{0}|x_{1})}\\big] \\ + \\ \\mathbb{E}_{q(x_{T}|x_{0})}\\big[\\text{log }\\frac{p(x_{T})}{q(x_{T}|x_{0})}\\big] \\ + \\  \\sum\\limits_{t=2}^{T}\\mathbb{E}_{q(x_{t},\\ x_{t-1}|x_{0})}\\bigg[\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\tag{9}\\\\\n","\\end{align*}\n","$$\n","<br/>\n","\n","\n","In equation *(9)* we used linearity of expectation to expand the terms and rearranged the subscripts accordingly. This has provided us an opportunity to express the expression `L` in terms of KL-divergence. Let's do that\n","\n","$$\n","\\begin{align*}\n","L &= \\mathbb{E}_{q(x_{1}|x_{0})}\\big[\\text{log }{p_{\\theta}(x_{0}|x_{1})}\\big] \\ + \\ \\mathbb{E}_{q(x_{T}|x_{0})}\\big[\\text{log }\\frac{p(x_{T})}{q(x_{T}|x_{0})}\\big] \\ + \\  \\sum\\limits_{t=2}^{T}\\mathbb{E}_{q(x_{t},\\ x_{t-1}|x_{0})}\\bigg[\\text{log}\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\\biggr] \\\\\n","\\Rightarrow L &= \\mathbb{E}_{q(x_{1}|x_{0})}\\big[\\text{log }{p_{\\theta}(x_{0}|x_{1})}\\big] \\ - \\ D_{KL}\\bigg(q(x_{T}|x_{0}) \\parallel \\ p(x_{T}) \\bigg) \\ - \\ \\sum\\limits_{t=2}^{T}\\mathbb{E}_{q(x_{t}|x_{0})}\\big[D_{KL}\\big(q(x_{t-1}|x_{t}, x_{0}) \\ \\parallel \\ p_{\\theta}(x_{t-1}|x_{t}) \\big] \\tag{10} \\\\\n","\\end{align*}\n","$$<br/>\n","\n","\n","\n","A few points to note about this equation:\n","\n","1. Take a look at the second term $D_{KL}\\big(q(x_{T}|x_{0}) \\parallel \\ p(x_{T}\\big)$. The forward process represented by `q` is fixed. The term $p(x_{T})$ is nothing but the end point of the forward process and the start of the reverse process, hence it is fixed as well. This means that we can safely ignore the second term in the optimization process. \n","2. Let's take a look at the KL-divergence involved in the third term $D_{KL}\\big(q(x_{t-1}|x_{t}, x_{0}) \\ \\parallel \\ p_{\\theta}(x_{t-1}|x_{t})$ This is known the *denoising matching term*. We proved earlier that if $x_{0}$ is known, we can show that the intermediate step in the forward process `q` are Gaussian (we will show it later). The reverse steps in `p` are also parameterized as Gaussian (check eq. (3)), hence we can say that the KL divergence term at each timestep is a comparison between two Gaussians.  The term $(q(x_{t-1}|x_{t}, x_{0})$ also serves as the GT since it defines how to denoise a noisy image ${x_{t}}$ with access to what the final, completely denoised image $x_{0}$ should be.\n","3. As a result of above two, we can rewrite our  training objective as:\n","$$\n","L = \\mathbb{E}_{q(x_{1}|x_{0})}\\big[\\text{log }{p_{\\theta}(x_{0}|x_{1})}\\big] \\ - \\ \\sum\\limits_{t=2}^{T}\\mathbb{E}_{q(x_{t}|x_{0})}\\big[D_{KL}\\big(q(x_{t-1}|x_{t}, x_{0}) \\ \\parallel \\ p_{\\theta}(x_{t-1}|x_{t}) \\big] - C \\tag{11} \\\\\n","$$\n","\n","<br/>\n","\n","Let's revisit equation (8) again now. We know that:\n","\n","$$\n","\\begin{align*}\n","q(x_{t-1} | x_{t}, x_{0}) &= q(x_{t} | x_{t-1}, x_{0}) \\frac{q(x_{t-1}| x_{0})}{q(x_{t} | x_{0})}\n","\\end{align*}\n","$$\n","\n","As proved earlier, we can sample any arbritrary forward step given $x_{0}$ as:\n","$$\n","q(x_{t}\\vert x_{0}) = \\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha_{t}}} x_{0},\\ (1 - \\bar{\\alpha_{t}}) I)\n","$$\n","\n","Combining the above two facts, we can say that:\n","\n","$$\n","\\begin{align*}\n","q(x_{t-1} | x_{t}, x_{0}) &= q(x_{t} | x_{t-1}, x_{0}) \\frac{q(x_{t-1}| x_{0})}{q(x_{t} | x_{0})} \\\\\n","&= \\frac{\\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha_{t}}} x_{0},\\ (1 - \\alpha_{t}) \\ I) \\ \\ \\mathcal{N}(x_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}} x_{0},\\ (1 - \\bar{\\alpha}_{t-1}) \\ I)}{\\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha_{t}}} x_{0},\\ (1 - \\bar{\\alpha_{t}}) \\ I)} \\\\\n","&\\propto \\text{exp} \\bigg\\{-\\frac{1}{2}\\biggl[\n","\\frac{(x_{t} - \\sqrt{\\alpha_{t}}x_{t-1})^2}{(1 \\ - \\ \\alpha_{t})} \\ + \\ \n","\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_{0})^2}{(1 \\ - \\ \\bar{\\alpha}_{t-1})} \\ - \\\n","\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t}}x_{0})^2}{(1 \\ - \\ \\bar{\\alpha}_{t})}\n","\\biggr]\\bigg\\} \\\\\n","&= \\text{exp} \\bigg\\{-\\frac{1}{2}\\biggl[\n","\\frac{(x_{t} - \\sqrt{\\alpha_{t}}x_{t-1})^2}{\\beta_{t}} \\ + \\ \n","\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_{0})^2}{(1 \\ - \\ \\bar{\\alpha}_{t-1})} \\ - \\\n","\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t}}x_{0})^2}{(1 \\ - \\ \\bar{\\alpha}_{t})}\n","\\biggr]\\bigg\\} \\\\\n","&= \\text{exp} \\bigg\\{-\\frac{1}{2}\\biggl[\n","\\frac{x_{t}^2 - 2\\sqrt{\\alpha_{t}}x_{t-1}x_{t} \\ + \\ \\alpha_{t}x_{t-1}^2}{\\beta_{t}} \\ + \\ \n","\\frac{x_{t-1}^2 - 2\\sqrt{\\bar{\\alpha}_{t-1}}x_{0}x_{t-1} \\ + \\ \\bar{\\alpha}_{t-1}x_{0}^2}{(1 \\ - \\ \\bar{\\alpha}_{t-1})} \\ - \\\n","\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t}}x_{0})^2}{(1 \\ - \\ \\bar{\\alpha}_{t})}\n","\\biggr]\\bigg\\} \\\\\n","&= \\text{exp} \\bigg\\{-\\frac{1}{2}\\biggl[\n","\\bigg(\\frac{\\alpha_{t}}{\\beta_{t}} \\ + \\ \\frac{1}{1 \\ - \\ \\bar{\\alpha}_{t-1}}\\bigg) x_{t-1}^2 \\ - \\\n","\\bigg(\\frac{2\\sqrt{\\alpha_{t}}\\ x_{t}}{\\beta_{t}} \\ + \\ \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}\\ x_{0}}{1 \\ - \\ \\bar{\\alpha}_{t-1}}\\bigg) x_{t-1} \\ + \\ C(x_{t}, x_{0}) \n","\\biggr]\\bigg\\} \\tag{12} \\\\\n","\\end{align*}\n","$$\n","<br/>\n","\n","The term $C(x_{t}, x_{0}$ is some value computed using $x_{t}, x_{0}, \\text{and} \\ \\alpha$, and doesn't involve $x_{t-1}$. Hence we treat it as a constant term w.r.t to $x_{t-1}$. We know that in the case of a standard Gaussian, the density is proportional to:\n","\n","$$\n","\\begin{align*}\n","&\\propto \\text{exp}\\big(- \\ \\frac{(x - \\mu)^2}{2\\sigma^2}\\big) \\\\\n","&=\\text{exp}\\big(- \\frac{1}{2} \\frac{(x^2 \\ + \\ \\mu^2 \\ - \\ 2\\mu x)}{\\sigma^2}\\big) \\tag{13} \\\\\n","\\end{align*}\n","$$\n","<br/>\n","\n","Comparing the coefficients of our Gaussian in **(12)** and the standard Gaussian in **(13)** we get:\n","\n","$$\n","\\begin{align*}\n","\\tilde{\\beta_{t}} &=\n","1 \\ \\big/ \\bigg(\\frac{\\alpha_{t}}{\\beta_{t}} \\ + \\ \\frac{1}{1 \\ - \\ \\bar{\\alpha}_{t-1}}\\bigg) = \n","\\frac{1- \\bar{\\alpha}_{t-1}}{1- \\bar{\\alpha}_t} \\beta_{t} \n","= \\frac{(1- \\bar{\\alpha}_{t-1})(1 - \\alpha_{t})}{1 - \\bar{\\alpha}_{t}} \\tag{14} \\\\ \\\\ \\\\\n","\\tilde{\\mu}(x_{t}, x_{0}) &= \n","\\frac\n","{\n","\\dfrac{\\sqrt{\\alpha_{t}}\\ x_{t}}{\\beta_{t}} \\ + \\ \\dfrac{\\sqrt{\\bar{\\alpha}_{t-1}}\\ x_{0}}{1 \\ - \\ \\bar{\\alpha}_{t-1}}\n","}\n","{\n","\\dfrac{\\alpha_{t}}{\\beta_{t}} \\ + \\ \\dfrac{1}{1 \\ - \\ \\bar{\\alpha}_{t-1}}\n","} \\\\ \\\\\n","&=\\dfrac{\\sqrt{\\alpha_{t}}\\ x_{t}}{\\beta_{t}} \\ + \\ \\dfrac{\\sqrt{\\bar{\\alpha}_{t-1}}\\ x_{0}}{1 \\ - \\ \\bar{\\alpha}_{t-1}} \\\\ \\\\\n","&=\\bigg(\n","\\dfrac{\\sqrt{\\alpha_{t}}\\ x_{t}}{\\beta_{t}} \\ + \\ \\dfrac{\\sqrt{\\bar{\\alpha}_{t-1}}\\ x_{0}}{1 \\ - \\ \\bar{\\alpha}_{t-1}}\n","\\bigg)\n","\\frac{1- \\bar{\\alpha}_{t-1}}{1- \\bar{\\alpha}_t} \\beta_{t} \\\\ \\\\\n","\\Rightarrow \\tilde{\\mu}(x_{t}, x_{0}) &= \n","\\frac{\\sqrt{\\alpha_{t}}(1- \\bar{\\alpha}_{t-1})}{1- \\bar{\\alpha}_{t}} x_{t} \\ + \\ \n","\\frac{\\sqrt{\\alpha_{t-1}}\\ \\beta_{t}}{1- \\bar{\\alpha}_{t}} x_{0} \\tag{15} \\\\\n","\\end{align*}\n","$$\n","<br/>\n","\n","When we applied the reparameterization trick, we learned that we can write:<br/>\n","$$\n","x_{t} =  \\sqrt{\\bar{\\alpha_{t}}}x_{0} + \\sqrt{1 - \\bar{\\alpha_{t}}}\\epsilon_{t} \\\\\n","$$\n","\n","Hence we can express *(15)* as:\n","\n","$$\n","\\begin{align*}\n","\\Rightarrow \\tilde{\\mu}(x_{t}, x_{0}) &= \n","\\frac{\\sqrt{\\alpha_{t}}(1- \\bar{\\alpha}_{t-1})}{1- \\bar{\\alpha}_{t}} x_{t} \\ + \\\n","\\frac{\\sqrt{\\alpha_{t-1}}\\ \\beta_{t}}{1- \\bar{\\alpha}_{t}}\n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} (x_{t} \\ - \\ \\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}\\epsilon_{t}) \\\\\n","\\Rightarrow \\tilde{\\mu}(x_{t}, x_{0}) &=\n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} \\bigg(x_{t} \\ - \\ \\frac{\\beta_{t}}{\\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}}\\epsilon_{t}\\bigg)\n","\\tag{16}\n","\\\\ \\\\\n","\\therefore \\ &q(x_{t-1} | x_{t}, x_{0}) \\sim \\mathcal {N}(x_{t-1}; \\tilde{\\mu}(x_{t}, x_{0}), \\ \\tilde{\\beta}\\bf{I})\n","\\end{align*}\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["So, $q(x_{t-1} | x_{t}, x_{0})$ is the true distribution, and $\\tilde{\\mu}(x_{t}, x_{0}),\\text{, and } \\tilde{\\beta}$ are the true parameters we are trying to approximate.\n","Let's talk about the other distriution  $p_{\\theta}(x_{t-1}, x_{t})$. Our model has to approximate the conditioned probability distributions in the reverse diffusion process:\n","\n","$$\n","p_{\\theta}(x_{t-1}|x_{t}) := \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t}, t))\n","$$\n","\n","Though we can try to learn both the mean and the variance of this distribution, the authors of the DDPMs paper found that learning the variance leads to lower quality samples. They decided to keep the variance $\\Sigma_{\\theta}(x_{t}, t)$ to time specific constants. Hence our network is solely tasked with learning the mean $\\mu_{\\theta}(x_{t}, t)$. The true mean that we are trying to approximate is:\n","\n","$$\n","\\begin{align*}\n","\\tilde{\\mu}(x_{t}, x_{0}) &=\n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} \\bigg(x_{t} \\ - \\ \\frac{\\beta_{t}}{\\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}}\\epsilon_{t}\\bigg) = \n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} \\bigg(x_{t} \\ - \\ \\frac{1 \\ - \\ \\alpha_{t}}{\\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}}\\epsilon_{t}\\bigg)\n","\\end{align*}\n","$$\n","\n","$x_{t}$ will be available as an input during training. $\\alpha_{t}$ is already known to us in advance (check the Reparameterization section), therefore we can reparameterize the Gaussian noise term instead to predict $\\epsilon_{t}$ at a given timestep `t` as:\n","\n","$$\n","\\begin{align*}\n","\\mu_{\\theta}(x_{t}, t) &=\n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} \\bigg(x_{t} \\ - \\ \\frac{1 \\ - \\ \\alpha_{t}}{\\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t}, t)\\bigg) \\tag{17}\n","\\end{align*}\n","$$\n","\n","Let's recap of all the things we covered in the last section:\n","1. We showed that $q(x_{t-1} | x_{t}, x_{0})$ is just a Gaussian, and that the KL-divergence at each timestep (for $t > 1$) between $q(x_{t-1} | x_{t}, x_{0})$ and $p_{\\theta}(x_{t-1}|x_{t})$ is KL-divergence between two Gaussian distributions\n","2. During training, we have kept the variances of the reverse process to time-specific constants. Our only task is to learn to approximate the mean during training.\n","3. With reparameterization, we can instead learn to predict the noise at each timestep. \n","\n","<br/>\n","\n","Let's revisit our objectve function now. Writing down the equation here:<br/>\n","$$\n","L = \\mathbb{E}_{q(x_{1}|x_{0})}\\big[\\text{log }{p_{\\theta}(x_{0}|x_{1})}\\big] \\ - \\ \\sum\\limits_{t=2}^{T}\\mathbb{E}_{q(x_{t}|x_{0})}\\big[D_{KL}\\big(q(x_{t-1}|x_{t}, x_{0}) \\ \\parallel \\ p_{\\theta}(x_{t-1}|x_{t}) \\big] \\ - C \\\\\n","$$\n","\n","We know that the KL divergence between two Gaussian distributions can be evaluated in closed form (Check out this excellent [post](https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/) for more details). Also, the variance is fixed, and we only need to minimize the difference between the actual mean and the predicted mean. \n","\n","$$\n","\\begin{align*}\n","L_{t\\sim U \\{ 2, T \\} } = \\mathbb{E}_{q}\\big[D_{KL}\\big(\n","\\mathcal {N}(x_{t-1}; \\tilde{\\mu}(x_{t}, x_{0}), \\ \\tilde{\\beta}{I}) \\ \\parallel \\ \n","\\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t}, t)\\big) \\big] \\\\\n","\\end{align*}\n","$$\n","<br/>\n","\n","Focusing on the KL divergence part:<br/>\n","$$\n","\\begin{align*}\n","&D_{KL}\\big(\n","\\mathcal {N}(x_{t-1}; \\tilde{\\mu}(x_{t}, x_{0}), \\ \\tilde{\\beta}{I}) \\ \\parallel \\ \n","\\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t}, t)\\big) \n","\\\\\n","&=\n","\\frac{1}{2} \\bigg[\\text{log}\\frac{\\bar{\\beta}}{\\Sigma_{\\theta}(x_{t}, t)} \\ - \\ d  \\ + \\\n","\\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)^{T} \\Sigma_{\\theta}(x_{t}, t)^{-1} \\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)\\big)\\bigg]\n","\\\\\n","\\because\\text{variances are kept fixed}\n","\\\\\n","&=\\frac{1}{2} \\bigg[\\text{log}\\frac{\\bar{\\beta}}{\\bar{\\beta}} \\ - \\ d  \\ + \\\n","\\text{tr}\\big({\\bar{\\beta}^{-1} {\\bar{\\beta}}}\\big) \\ + \\\n","\\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)^{T} \\bar{\\beta}^{-1} \\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)\\big)\\bigg]  \\ \\ \\ \\ \\ \n","\\\\\n","&=\\frac{1}{2} \\bigg[\\text{log}1 \\ - \\ d  \\ + \\ d \\ + \\\n","\\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)^{T} \\bar{\\beta}^{-1} \\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)\\big)\\bigg]\n","\\\\\n","&=\\frac{1}{2\\bar{\\beta}} \\bigg[\n","\\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)^{T} \\big(\\bar\\mu(x_{t}, x_{0}) \\ - \\  \\mu_{\\theta}(x_{t}, t)\\big)\\big)\\bigg]\n","\\end{align*}\n","$$\n","<br/><br/>\n","\n","Now that we have expanded the KL divergence, we can rewrite our loss function as: <br/>\n","\n","$$\n","\\begin{align*}\n","\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n","L_{t\\sim U \\{ 2, T \\} } &= \\mathbb{E}_{x_{0}, \\epsilon, t}\\bigg[\n","\\dfrac{1}{2\\bar{\\beta}}\n","\\norm{\\tilde{\\mu}_{\\theta}(x_{t}, x_{0})\\ - \\ \\mu_{\\theta}(x_{t}, t)}^2\n","\\bigg] \\\\\n","&=\\mathbb{E}_{x_{0}, \\epsilon, t}\\bigg[\n","\\dfrac{1}{2 \\bar{\\beta}}\n","\\norm{\n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} \\bigg(x_{t} \\ - \\ \n","\\frac{1 \\ - \\ \\alpha_{t}}{\\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}}\\epsilon_{t})\\bigg) \\ - \\\n","\\frac{1}{\\sqrt{\\bar{\\alpha}}_{t}} \\bigg(x_{t} \\ - \\ \n","\\frac{1 \\ - \\ \\alpha_{t}}{\\sqrt{1 \\ - \\ \\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t}, t)\\bigg)\n","}^2\n","\\bigg] \\\\\n","&=\\mathbb{E}_{x_{0}, \\epsilon, t}\\bigg[\n","\\underbrace{\\dfrac{(1 \\ - \\ \\alpha_{t})^2}{2 \\alpha_{t} (1 \\ - \\ \\alpha_{t})\\bar{\\beta}}}_{\\text{step-specific weighting}}\n","\\norm{\\epsilon_t \\ - \\ \\epsilon_{\\theta}(x_{t}, t)}^2\n","\\bigg] \\tag{18}\n","\\end{align*}\n","$$\n","<br/>\n","\n","<br/> A simpler version was also proposed by the authors, where they also discard this step-specific weigthing term. We can rewrite our final loss function as: <br/>\n","\n","$$\n","\\begin{align*}\n","L^{simple} &= \\mathbb{E}_{x_{0}, \\epsilon, t}\\big[\n","\\norm{\\epsilon_t \\ - \\ \\epsilon_{\\theta}(x_{t}, t)}^2\n","\\big]\n","\\end{align*}\n","$$\n","\n","<br/> The final objective function can be written as: <br/>\n","$$\n","\\begin{align*}\n","L = L^{simple} \\  +  \\ C \\tag{19}\n","\\end{align*}\n","$$\n","\n","where **C** is a constant not depending on $\\theta$"]},{"cell_type":"markdown","metadata":{},"source":["That's it for now. I hope this notebook was enough to give you a solid understanding of the Diffusion models and the underlying maths. I tried to break down the maths as much as possible. If you liked this work, please consider giving an **upvote**!\n","\n","\n","# References\n","\n","1. https://blog.evjang.com/2016/08/variational-bayes.html\n","2. https://lilianweng.github.io/posts/2018-08-12-vae/\n","3. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n","4. https://www.youtube.com/watch?v=fbLgFrlTnGU\n","5. https://arxiv.org/pdf/2208.11970.pdf\n","6. https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/\n","7. https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}

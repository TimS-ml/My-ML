{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af72596c-647a-4da1-985a-1ab8e1722748",
   "metadata": {},
   "source": [
    "# RNN for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3aa4f-0e99-4a9f-afce-38a2aec4887a",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21130b64-8831-42df-9aa9-ec8b933c913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code is from https://github.com/spro/practical-pytorch\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from name_dataset import NameDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4380de2c-fa22-4458-aa6a-2732815af15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and DataLoaders\n",
    "HIDDEN_SIZE = 100\n",
    "N_CHARS = 128  # ASCII\n",
    "N_CLASSES = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702a1568-1437-4670-af1d-899208ab4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "\n",
    "        # input = B x S . size(0) = B\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # input:  B x S  -- (transpose) --> S x B\n",
    "        input = input.t()\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        print(\"  input\", input.size())\n",
    "        embedded = self.embedding(input)\n",
    "        print(\"  embedding\", embedded.size())\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(\"  gru hidden output\", hidden.size())\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden)\n",
    "        print(\"  fc output\", fc_output.size())\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc17a10-cf84-473c-bda7-8c5800dbe77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help functions\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor\n",
    "\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798bcc56-69e0-4b67-992c-bf02348033ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  input torch.Size([6, 1])\n",
      "  embedding torch.Size([6, 1, 100])\n",
      "  gru hidden output torch.Size([1, 1, 100])\n",
      "  fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 6]) out torch.Size([1, 1, 18])\n",
      "  input torch.Size([5, 1])\n",
      "  embedding torch.Size([5, 1, 100])\n",
      "  gru hidden output torch.Size([1, 1, 100])\n",
      "  fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 5]) out torch.Size([1, 1, 18])\n",
      "  input torch.Size([4, 1])\n",
      "  embedding torch.Size([4, 1, 100])\n",
      "  gru hidden output torch.Size([1, 1, 100])\n",
      "  fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 4]) out torch.Size([1, 1, 18])\n",
      "  input torch.Size([3, 1])\n",
      "  embedding torch.Size([3, 1, 100])\n",
      "  gru hidden output torch.Size([1, 1, 100])\n",
      "  fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 3]) out torch.Size([1, 1, 18])\n",
      "  input torch.Size([6, 4])\n",
      "  embedding torch.Size([6, 4, 100])\n",
      "  gru hidden output torch.Size([1, 4, 100])\n",
      "  fc output torch.Size([1, 4, 18])\n",
      "batch in torch.Size([4, 6]) batch out torch.Size([1, 4, 18])\n"
     ]
    }
   ],
   "source": [
    "names = ['adylov', 'solan', 'hard', 'san']\n",
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
    "\n",
    "for name in names:\n",
    "    arr, _ = str2ascii_arr(name)\n",
    "    inp = Variable(torch.LongTensor([arr]))\n",
    "    out = classifier(inp)\n",
    "    print(\"in\", inp.size(), \"out\", out.size())\n",
    "\n",
    "inputs = make_variables(names)\n",
    "out = classifier(inputs)\n",
    "print(\"batch in\", inputs.size(), \"batch out\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d50d5-89bd-4785-b007-da95def0f930",
   "metadata": {},
   "source": [
    "## Advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b931c8d-0b6a-48aa-8e8a-f13ade7f0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and DataLoaders\n",
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75a236-b90b-44b9-ba45-d8de4052cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NameDataset(is_train_set=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True)\n",
    "\n",
    "train_dataset = NameDataset(is_train_set=True)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "\n",
    "N_COUNTRIES = len(train_dataset.get_countries())\n",
    "print(N_COUNTRIES, \"countries\")\n",
    "N_CHARS = 128  # ASCII\n",
    "\n",
    "\n",
    "# Some utility functions\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def create_variable(tensor):\n",
    "    # Do cuda() before wrapping with variable\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # Sort tensors by their length\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "    # Also sort the target (countries) in the same order\n",
    "    target = countries2tensor(countries)\n",
    "    if len(countries):\n",
    "        target = target[perm_idx]\n",
    "\n",
    "    # Return variables\n",
    "    # DataParallel requires everything to be a Variable\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names, countries):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
    "\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "def countries2tensor(countries):\n",
    "    country_ids = [\n",
    "        train_dataset.get_country_id(country) for country in countries\n",
    "    ]\n",
    "    return torch.LongTensor(country_ids)\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 n_layers=1,\n",
    "                 bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size,\n",
    "                          hidden_size,\n",
    "                          n_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        # input shape: B x S (input size)\n",
    "        # transpose to make S(sequence) x B (batch)\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # Pack them up nicely\n",
    "        gru_input = pack_padded_sequence(embedded,\n",
    "                                         seq_lengths.data.cpu().numpy())\n",
    "\n",
    "        # To compact weights again call flatten_parameters().\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size,\n",
    "                             self.hidden_size)\n",
    "        return create_variable(hidden)\n",
    "\n",
    "\n",
    "# Train cycle\n",
    "def train():\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (names, countries) in enumerate(train_loader, 1):\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        classifier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                '[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
    "                    time_since(start), epoch, i * len(names),\n",
    "                    len(train_loader.dataset),\n",
    "                    100. * i * len(names) / len(train_loader.dataset),\n",
    "                    total_loss / i * len(names)))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Testing cycle\n",
    "def test(name=None):\n",
    "    # Predict for a given name\n",
    "    if name:\n",
    "        input, seq_lengths, target = make_variables([name], [])\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        country_id = pred.cpu().numpy()[0][0]\n",
    "        print(name, \"is\", train_dataset.get_country(country_id))\n",
    "        return\n",
    "\n",
    "    print(\"evaluating trained model ...\")\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "\n",
    "    for names, countries in test_loader:\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093e40e-4d81-41f0-b90d-54a4005ea93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
    "    classifier = nn.DataParallel(classifier)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    classifier.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    # Train cycle\n",
    "    train()\n",
    "\n",
    "    # Testing\n",
    "    test()\n",
    "\n",
    "    # Testing several samples\n",
    "    test(\"Sung\")\n",
    "    test(\"Jungwoo\")\n",
    "    test(\"Soojin\")\n",
    "    test(\"Nako\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x01 Difference between pt and quickstart.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giHLbvSllx1V"
      },
      "source": [
        "# Difference between Pytorch and Tensorflow\n",
        "\n",
        "https://towardsdatascience.com/pytorch-vs-tensorflow-in-code-ada936fd5406\n",
        "\n",
        "http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture07.pdf\n",
        "\n",
        "http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture08.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLeHZBpjx4Gz"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z948Ddpqvb5p"
      },
      "source": [
        "import io\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/TimS-ml/DataMining/master/z_Other/tweets.csv'\n",
        "\n",
        "f = requests.get(url).content\n",
        "df = pd.read_csv(io.StringIO(f.decode('utf-8')))\n",
        "df = df.iloc[:, 1:]\n",
        "df.columns = ['sentiments', 'tweets']\n",
        "\n",
        "# df.shape  # (31962, 2)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2LXiSccy8tl"
      },
      "source": [
        "# instantiate and fit tokenizer\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token='<00v>')\n",
        "tokenizer.fit_on_texts(df.tweets)\n",
        "\n",
        "# transform tweets into sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(df.tweets)\n",
        "\n",
        "# pad sequences so that they have uniform lenth\n",
        "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=42)\n",
        "assert(padded.shape==(31962, 42))\n",
        "\n",
        "seq = padded\n",
        "labels = np.array(df.sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b7ik9-voHXm"
      },
      "source": [
        "# Pytorch\n",
        "\n",
        "There are two ways to build a neural network model in PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HANalHbpm5p"
      },
      "source": [
        "## Two ways of building NN in PT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-1R6zDtJ-N"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KCSLl3eo6vw"
      },
      "source": [
        "### [1] Model Subclassing\n",
        "Similar to TensorFlow, in PyTorch you subclass the `nn.Model` module and define your layers in the `__init__()` method. \n",
        "\n",
        "The only difference is that you create the `forward` pass in a method named forward *instead of `call`*.\n",
        "\n",
        "Difference to the Keras model: <u>There’s only an average-pooling layer in PyTorch so it needs to have the right kernel size in order the make it global average-pooling.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5LxP3IZoskt"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=20000,\n",
        "                                            embedding_dim=50)\n",
        "        self.pooling_layer = nn.AvgPool1d(kernel_size=50)\n",
        "        self.fc_layer = nn.Linear(in_features=42, out_features=1)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.pooling_layer(x).view(32, 42)\n",
        "        return torch.sigmoid(self.fc_layer(x))\n",
        "    \n",
        "model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQwfyIHYo02W"
      },
      "source": [
        "### [2] Sequential\n",
        "PyTorch also offers a `Sequential` module that looks almost equivalent to TensorFlow’s.\n",
        "\n",
        "Many layers do not work with PyTorch’s `nn.Sequential`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P110pyzxp3Q2"
      },
      "source": [
        "# PyTorch nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(num_embeddings=20000, embedding_dim=50),\n",
        "    nn.AvgPool1d(kernel_size=50),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(in_features=42, out_features=1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OoBbFDz1LN"
      },
      "source": [
        "## Training a NN in PT\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "\n",
        "Training loop needs to be implemented from scratch\n",
        "\n",
        "In oder to process the data in batches, a dataloader must be created. The dataloader returns one batch at a time in a dictionary format.\n",
        "\n",
        "Short description of the training loop: \n",
        "- For each batch, we calculate the loss and then call loss.backward() to backpropagate the gradient through the layers. \n",
        "- In addition, we call optimizer.step() to tell the optimizer to update the parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOrmaWNVqdMS"
      },
      "source": [
        "# define the loss fn and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# initialize empty list to track batch losses\n",
        "batch_losses = []\n",
        "\n",
        "# train the neural network for 5 epochs\n",
        "for epoch in range(5):\n",
        "    # reset iterator\n",
        "    dataiter = iter(dataloader)\n",
        "    \n",
        "    for batch in dataiter:\n",
        "        # reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward propagation through the network\n",
        "        out = model(batch[\"tweets\"])\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(out, batch[\"sentiments\"])\n",
        "        \n",
        "        # track batch loss\n",
        "        batch_losses.append(loss.item())\n",
        "        \n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # update the parameters\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_EnL-lnHSN"
      },
      "source": [
        "# Tensorflow\n",
        "\n",
        "TensorFlow is a lot like Scikit-Learn thanks to its `fit` function, which makes training a model super easy and quick.\n",
        "\n",
        "There are three ways to build a neural network model in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu-Cfjwtq7h6"
      },
      "source": [
        "## Three ways of building NN in TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THDUGfhivjlq"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygwFVFKDrG-H"
      },
      "source": [
        "\n",
        "### [1] Model subclassing\n",
        "\n",
        "You can create your own fully-customizable models by subclassing the `tf.keras.Model` class and implementing the forward pass in the `call` method. \n",
        "\n",
        "Put differently, layers are defined in the __init__() method and the logic of the forward pass in the call method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ek17JFhrPRu"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim=20000,\n",
        "                                                         output_dimension=50,\n",
        "                                                         input_length=42,\n",
        "                                                         mask_zero=True)\n",
        "        self.flatten_layer = tf.keras.layers.Flatten()\n",
        "        self.fc1_layer =  tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.fc2_layer =  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.flatten_layer(x)\n",
        "        x = self.fc1_layer(x)\n",
        "        return self.fc2_layer(x)\n",
        "        \n",
        "model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrdFWoSrUO-"
      },
      "source": [
        "\n",
        "### [2] Functional API\n",
        "Given some input tensor(s) and output tensor(s), you can also instantiate 实例化 a `Model`. \n",
        "\n",
        "With this approach, you essentially define a layer and immediately pass it the input of the previous layer. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDF4d0uDra1D"
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(42,))\n",
        "x = tf.keras.layers.Embedding(input_dim=20000,\n",
        "                              output_dimension=50,\n",
        "                              input_length=42,\n",
        "                              mask_zero=True)(inputs)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsZiUiOergva"
      },
      "source": [
        "### [3] Sequential model API\n",
        "Typically consisting of just a few common layers — kind of a shortcut to a trainable model. \n",
        "\n",
        "Too inflexible if you wish to implement more sophisticated ideas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2M2t9CerGam"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=20000,\n",
        "                              output_dimension=50,\n",
        "                              input_length=42,\n",
        "                              mask_zero=True),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDFCfDxvwzUq"
      },
      "source": [
        "## Two useful functions of TF\n",
        "\n",
        "First, calling `model.summary`() prints a compact summary of the model and the number of parameters\n",
        "\n",
        "Second, by calling `tf.keras.utils.plot_model()` you get a graphical summary of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GfIbncgxM9-"
      },
      "source": [
        "## Training a NN in Keras\n",
        "\n",
        "Before you can train a Keras model, it must be compiled by running the `model.compile()` function, which is also where you specify the loss function and optimizer.\n",
        "\n",
        "```python\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "Keras models have a convenient `model.fit()` function for training a model (just like Scikit-Learn), which also takes care of batch processing and even evaluates the model on the run (if you tell it to do so).\n",
        "\n",
        "```python\n",
        "model.fit(x=X, y, batch_size=32, epochs=5, verbose=2, validation_split=0.2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkfM9VDazycJ"
      },
      "source": [
        "## Keras Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04QgGZc9bF5D"
      },
      "source": [
        "This short introduction uses [Keras](https://www.tensorflow.org/guide/keras/overview) to:\n",
        "\n",
        "1. Build a neural network that classifies images.\n",
        "2. Train this neural network.\n",
        "3. And, finally, evaluate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trJmd6DjqBZ"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NAbSZiaoJ4z"
      },
      "source": [
        "Load and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Convert the samples from integers to floating-point numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FP5258xjs-v"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "Build the `tf.keras.Sequential` model by stacking layers. Choose an optimizer and loss function for training:\n",
        "\n",
        "tf.keras.models.Sequential\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/sequential_model\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "\n",
        "\n",
        "tf.keras.layers.Flatten\n",
        "\n",
        "Flattens the input. Does not affect the batch size.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "\n",
        "\n",
        "tf.keras.layers.Dense\n",
        "\n",
        "Just your regular densely-connected NN layer.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "\n",
        "Dense implements the operation: \n",
        "- output = activation(dot(input, kernel) + bias) \n",
        "- where activation is the element-wise activation function passed as the activation argument,\n",
        "- kernel is a weights matrix created by the layer, \n",
        "- bias is a bias vector created by the layer (only applicable if use_bias is True)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2hiez2eIUz8"
      },
      "source": [
        "For each example the model returns a vector of \"[logits](https://developers.google.com/machine-learning/glossary#logits)\" or \"[log-odds](https://developers.google.com/machine-learning/glossary#log-odds)\" scores, one for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeOrNdnkEEcR"
      },
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgjhDQGcIniO"
      },
      "source": [
        "The `tf.nn.softmax` function converts these logits to \"probabilities\" for each class: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWSRnQ0WI5eq"
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he5u_okAYS4a"
      },
      "source": [
        "Note: It is possible to bake this `tf.nn.softmax` in as the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to\n",
        "provide an exact and numerically stable loss calculation for all models when using a softmax output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyugpgRIyrA"
      },
      "source": [
        "The `losses.SparseCategoricalCrossentropy` loss takes a vector of logits and a `True` index and returns a scalar loss for each example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfR4MsSDU880"
      },
      "source": [
        "This loss is equal to the negative log probability of the true class:\n",
        "It is zero if the model is sure of the correct class.\n",
        "\n",
        "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf.log(1/10) ~= 2.3`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJWqEVrrJ7ZB"
      },
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9foNKHzTD2Vo"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix4mEL65on-w"
      },
      "source": [
        "The `Model.fit` method adjusts the model parameters to minimize the loss: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7suUbJXVLqP"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDAAPFqVVgn"
      },
      "source": [
        "The `Model.evaluate` method checks the models performance, usually on a \"[Validation-set](https://developers.google.com/machine-learning/glossary#validation-set)\" or \"[Test-set](https://developers.google.com/machine-learning/glossary#test-set)\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dTAzgHDUh7"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4JfEh7kvx6m"
      },
      "source": [
        "The image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj8NrlzlJqDG"
      },
      "source": [
        "If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnqOZtUp1YR_"
      },
      "source": [
        "probability_model(x_test[:5])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
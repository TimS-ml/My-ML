{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "0x01 Loss Graph and Auto Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTRNEEWDUX-v"
      },
      "source": [
        "# Desc\n",
        "\n",
        "[PyTorchZeroToAll](https://www.youtube.com/playlist?list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m)\n",
        "\n",
        "[Official Recipes](https://pytorch.org/tutorials/recipes/recipes_index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM98Z3YgEu8Y"
      },
      "source": [
        "http://cs231n.stanford.edu/slides/2020/lecture_6.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBdRea85Utcz"
      },
      "source": [
        "## Automatic Differentiation\n",
        "https://blog.csdn.net/aws3217150/article/details/70214422\n",
        "\n",
        "https://www.youtube.com/watch?v=ma2KXWblllc&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTS1bVxGGgb1"
      },
      "source": [
        "# Computational Graphs\n",
        "\n",
        "https://github.com/pytorch/pytorch/issues/5059\n",
        "\n",
        "https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c\n",
        "\n",
        "https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_computational_graphs.htm\n",
        "\n",
        "```\n",
        "x y z\n",
        "\\*/ |\n",
        " a  |\n",
        " \\+/\n",
        "  b\n",
        "  | sum\n",
        "  c\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYlSkNyfGh26"
      },
      "source": [
        "## Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPaIwa6pHDtO",
        "outputId": "a70aec29-a2d5-4bc1-a683-e5e685092948"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "N, D = 3, 4\n",
        "\n",
        "x = np.random.randn(N, D)\n",
        "y = np.random.randn(N, D)\n",
        "z = np.random.randn(N, D)\n",
        "# print(x, '\\n', y, '\\n', z)\n",
        "\n",
        "a = x * y\n",
        "b = a + z\n",
        "c = np.sum(b)\n",
        "\n",
        "grad_c = 1.0\n",
        "grad_b = grad_c * np.ones((N, D))\n",
        "grad_a = grad_b.copy()\n",
        "grad_z = grad_b.copy()\n",
        "grad_x = grad_a * y\n",
        "grad_y = grad_a * x\n",
        "\n",
        "# print(sum(sum(grad_x * x)))\n",
        "# print(sum(sum(grad_y * y)))\n",
        "# print(sum(sum(grad_z * z)))\n",
        "print(c)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.717008537800067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXBm-9qnGjvb"
      },
      "source": [
        "## Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAHj5FxlJgOc",
        "outputId": "61f3d151-a4e1-4928-b81d-974e71843021"
      },
      "source": [
        "import torch\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "N, D = 3, 4\n",
        "\n",
        "# t_x = torch.randn(N, D)\n",
        "# t_y = torch.randn(N, D)\n",
        "# t_z = torch.randn(N, D)\n",
        "t_x = torch.tensor(x)\n",
        "t_y = torch.tensor(y)\n",
        "t_z = torch.tensor(z)\n",
        "# print(t_x, '\\n', t_y, '\\n', t_z)\n",
        "\n",
        "a = t_x * t_y\n",
        "b = a + t_z\n",
        "c = torch.sum(b)\n",
        "\n",
        "print(c)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6.7170, dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGkjXTnKh7V"
      },
      "source": [
        "# Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagBDrdh0ivN"
      },
      "source": [
        "## Model Framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECe0aKaYn9m9"
      },
      "source": [
        "# guess w only\n",
        "# y = x * w\n",
        "# real: y = 2x\n",
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "# Loss manually\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) ** 2\n",
        "\n",
        "# backword manually\n",
        "# compute gradient\n",
        "# y_pred = x * w\n",
        "# loss = (y_pred - y) ** 2\n",
        "# d_loss/d_w = 2 * x * (x*w - y)\n",
        "def gradient(x, y):  # d_loss/d_w\n",
        "    return 2 * x * (x*w - y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TfcJBEIn9mr"
      },
      "source": [
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgmuoNkRn9nG"
      },
      "source": [
        "## loss graph (w and mse)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW5C7snGn9mg"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_3mNSFsn9nI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "601d7106-f217-483f-a00a-1276a8e9a602"
      },
      "source": [
        "# List of weights / Mean square Error (Mse) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "# w is random guess to predit y_pred\n",
        "for w in np.arange(0.0, 4.1, 0.2):\n",
        "    # Print the weights and initialize the lost\n",
        "    print(\"w =\", w)\n",
        "    l_sum = 0\n",
        "\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # For each input and output, calculate y_hat\n",
        "        # Compute the total loss and add to the total error\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)  # which is square error\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    # Now compute the Mean squared error (mse) of each\n",
        "    # Aggregate the weight/mse from this run\n",
        "    print(\"MSE =\", l_sum / 3)\n",
        "    print()\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE = 18.666666666666668\n",
            "\n",
            "w = 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE = 15.120000000000003\n",
            "\n",
            "w = 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE = 11.946666666666667\n",
            "\n",
            "w = 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE = 9.146666666666663\n",
            "\n",
            "w = 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE = 6.719999999999999\n",
            "\n",
            "w = 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE = 4.666666666666667\n",
            "\n",
            "w = 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE = 2.986666666666665\n",
            "\n",
            "w = 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE = 1.6799999999999995\n",
            "\n",
            "w = 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE = 0.746666666666666\n",
            "\n",
            "w = 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE = 0.1866666666666665\n",
            "\n",
            "w = 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE = 0.0\n",
            "\n",
            "w = 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE = 0.18666666666666698\n",
            "\n",
            "w = 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE = 0.7466666666666679\n",
            "\n",
            "w = 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE = 1.6800000000000008\n",
            "\n",
            "w = 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE = 2.986666666666668\n",
            "\n",
            "w = 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE = 4.666666666666667\n",
            "\n",
            "w = 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE = 6.720000000000003\n",
            "\n",
            "w = 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE = 9.14666666666667\n",
            "\n",
            "w = 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE = 11.94666666666667\n",
            "\n",
            "w = 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE = 15.120000000000005\n",
            "\n",
            "w = 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE = 18.666666666666668\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0eG5RR3n9nR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ab9afdb4-cd2d-40d2-90f1-9dbf0978c285"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss graph\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9bnH8c+TjRAIa8KahB1kkTUsLrhj0SpYN0Bq1VqpXW7rcturvbfV2l5ve9titfaqgFRtQcEdBRdcKihr2CNgCCFkYUlICAkJ2Z/7RwYb4wRCyMxvMvO8X695ceacM3O+HJh55pzz+/2OqCrGGGNMQ2GuAxhjjAlMViCMMcZ4ZQXCGGOMV1YgjDHGeGUFwhhjjFcRrgO0pLi4OO3bt6/rGMYY02ps2rTpiKrGe1sWVAWib9++pKSkuI5hjDGthojsb2yZnWIyxhjjlRUIY4wxXlmBMMYY45UVCGOMMV5ZgTDGGOOVFQhjjDFeWYEwxhjjVcgXiPKqGuavymDN3iOuoxhjzBn7eHcef/tsH5XVtS3+3iFfIMLDhPmrM5i/KsN1FGOMOWP/9890nl+TSUSYtPh7h3yBiAwPY8b4RP6Zlk/O0TLXcYwxpsnSDpewMfMosyYkEWYFwjdmTkhCgJc2ZLuOYowxTbZ4fRZR4WHcOC7BJ+9vBQLo3aktlw7pxksbs6mqafnzeMYY09LKKqt5dXMOV53bg67t2/hkG1YgPGZPSuLI8QpW7jzsOooxxpzW29sOUlJezeyJfXy2DSsQHhcP7kbvTm1ZvD7LdRRjjDmtRRuyGNitPeP7dvbZNqxAeISHCTPHJ/Jp+hH2HSl1HccYYxqVmnuMbdlFzJ6YhEjLX5w+yQpEPTPGJxIeJry4wY4ijDGBa9H6LKIjw7h+jG8uTp9kBaKebh2imTK0Oy+nZFNRXeM6jjHGfM3ximqWbc3lmpG96BgT6dNt+axAiMhCEckTkdR685aIyFbPI1NEtjby2kwR2eFZz6+3iJs9KYmjZVW8m3rIn5s1xpgmeWNLLqWVNcyemOTzbfnyCOI5YGr9Gao6Q1VHq+po4FXgtVO8/lLPusk+zPg1FwyIo0/XGBats9NMxpjAoqosWp/FsJ4dGJ3Yyefb81mBUNVVQKG3ZVJ3VeVm4EVfbb+5wsKEWyYksSGzkLTDJa7jGGPMl7ZkF7HrYDGzJ/n24vRJrq5BTAYOq+qeRpYr8L6IbBKROX7MBcCN4xKICg+zJq/GmICyeH0W7aLCmT66t1+256pAzOLURw8XqupY4CrgRyJyUWMrisgcEUkRkZT8/PwWCde1fRumjujBq5tzOFFpF6uNMe4dK6virW0HmD6mN+3bRPhlm34vECISAVwPLGlsHVXN9fyZB7wOTDjFuvNUNVlVk+Pj41ss5+yJSZSUV/PW9gMt9p7GGNNcr27OoaK6llsm+P7i9EkujiCuAHarao63hSLSTkRiT04DVwKp3tb1pQn9ujCwW3s7zWSMcU5VWbwhi1GJnRjRu6PftuvLZq4vAmuBISKSIyJ3ehbNpMHpJRHpJSIrPE+7A5+KyDZgA7BcVd/1Vc7GiNRdrN6aXURq7jF/b94YY760YV8h6XnH/dK0tT6fnchS1VmNzL/dy7wDwNWe6QxglK9ynYkbxibw+3d3s3hDFo9+61zXcYwxIWrR+ixioyO4dmQvv27XelKfQseYSK4d1Ys3t+RyvKLadRxjTAgqOF7BO6kHuWFsAm2jwv26bSsQp3HLxCRKK2t4c2uu6yjGmBD0yqYcqmrU76eXwArEaY1J7MTQnh34x7osVNV1HGNMCKmtrbs4PaFvFwZ1j/X79q1AnIaIMHtiErsOFrM1u8h1HGNMCPls7xH2F5Qxe5L/jx7ACkSTXDemN+2iwq3JqzHGrxavz6JLuyimjujhZPtWIJqgfZsIpo3uzVvbD3CsrMp1HGNMCDhcXM77Ow9z47gE2kT49+L0SVYgmmj2xCTKq2p5bYvX/n3GGNOilm7MpqZWmeXHntMNWYFoohG9OzIqsROL1tvFamOMb9XUKi9uyOLCgXH0i2vnLIcViDMwe0IS6XnH2Zh51HUUY0wQ+yQtjwPHyrnFQdPW+qxAnIFrRvUkNjqCRev3u45ijAlii9ZlER/bhinDujvNYQXiDMRERXDD2ATe2XGIwtJK13GMMUEot+gEH3+Rx4zkRCLD3X5FW4E4Q7dMTKKyppZXNmW7jmKMCUJLNmShwMwJia6jWIE4U4O7xzK+b2cWr8+ittYuVhtjWk5VTS0vbczmksHxJHSOcR3HCkRzzJ7Yh8yCMtbsLXAdxRgTRD7cdZi8kgpmT+zjOgpgBaJZpo7oQeeYSLtYbYxpUYvWZ9GrYzSXntPNdRTACkSzREeGc+O4BFbuPExecbnrOMaYILC/oJTVe44wY3wS4WHiOg5gBaLZZk1IorpWWZpiF6uNMWdv8YYswsOEGePdX5w+yQpEM/WPb88FA7vy4oa67vDGGNNcFdU1vJySwxVDu9GjY7TrOF/y5T2pF4pInoik1pv3sIjkishWz+PqRl47VUS+EJF0EXnAVxnP1uyJfcgtOsEnaXmuoxhjWrF3U+v6VgXKxemTfHkE8Rww1cv8x1R1tOexouFCEQkH/gpcBQwDZonIMB/mbLYpw7oT174N/1hnw4AbY5pv0boskrrEcOHAONdRvsJnBUJVVwGFzXjpBCBdVTNUtRJ4CZjeouFaSGR4GLMnJvHR7jzS8467jmOMaYW2ZRexIbOQWyf1ISxALk6f5OIaxI9FZLvnFFRnL8t7A/Wv/OZ45nklInNEJEVEUvLz81s662ndel4f2kSE8eynGX7ftjGm9Zu/OoPYNhEB0XO6IX8XiKeAAcBo4CDwp7N9Q1Wdp6rJqpocHx9/tm93xuLat+GGcQm8ujmX/JIKv2/fGNN6ZReWsWLHQW6ZmERsdKTrOF/j1wKhqodVtUZVa4H51J1OaigXqF9KEzzzAtadF/ajqqaWv6/NdB3FGNOKLPxsH2Ei3H5BX9dRvPJrgRCRnvWefgtI9bLaRmCQiPQTkShgJrDMH/maa0B8e64Y2p0X1u3nRGWN6zjGmFbgWFkVSzZmM21UL3p2bOs6jle+bOb6IrAWGCIiOSJyJ/C/IrJDRLYDlwL3etbtJSIrAFS1Gvgx8B6wC1iqqp/7KmdLmXNRf4rKqmyUV2NMkyzasJ+yyhq+N7m/6yiNivDVG6vqLC+zn21k3QPA1fWerwC+1gQ2kCX36czoxE4s+HQft0zsEzBd5Y0xgaeiuobnPstk8qA4hvXq4DpOo6wndQsREeZc1J/9BWWs3HnIdRxjTABbtvUAeSUV3BXARw9gBaJFfWN4DxK7tGXeKmvyaozxTlWZvzqDc3rEMnlQYHWMa8gKRAsKDxO+d2F/NmcVsWl/c/oIGmOC3Sdp+aQdPs5dk/sjEtinoq1AtLCbkhPo2DbSjiKMMV7NX51B9w5tuHZUL9dRTssKRAuLiYrg1kl9eH/nYTKPlLqOY4wJIJ8fOMZn6QXccUE/oiIC/+s38BO2Qt85vw+RYWE8++k+11GMMQFkwep9tIsKZ9aEJNdRmsQKhA90i43mW2N68/KmbApLK13HMcYEgANFJ3hr2wFmTkiiY9vAG1bDGysQPvK9yf0or6rlH+vsvtXGGHhuTSYK3BGgw2p4YwXCRwZ1j+Wyc7rx/JpMyqts+A1jQllxeRWL12fxzXN7ktA5xnWcJrMC4UPfm9yPgtJKXt8S0GMNGmN8bMmGbI5XVAd8x7iGrED40Hn9uzKidwfmr86g1u5bbUxIqqqpZeFn+5jUvwvnJnR0HeeMWIHwIRHhrsn9ycgv5aPddt9qY0LR8u0HOXisnDkXta6jB7AC4XNXn9uT3p3aMm+1dZwzJtSoKvNWZTCwW3suGdzNdZwzZgXCxyLDw7jjgr5s2FfI1uwi13GMMX60Zm8BOw8Wc9fkfgF3v+mmsALhBzMnJBEbHcF8O4owJqTMW5VBXPs2TB/d23WUZrEC4Qft20Rwy8Qk3tlxkOzCMtdxjDF+8MWhEj5Jy+f28/sQHRnuOk6zWIHwkzvO70eYiA2/YUyImL86g7aR4cye2Md1lGbz5S1HF4pInoik1pv3BxHZLSLbReR1EenUyGszPbcm3SoiKb7K6E89OkYzbXQvlqZkU1Rmw28YE8wOF5fz5tZcbk5OoHO7KNdxms2XRxDPAVMbzFsJjFDVkUAa8OApXn+pqo5W1WQf5fO7uyb3p6yyhkXrs1xHMcb40HNrMqmpVb57YT/XUc6KzwqEqq4CChvMe19Vqz1P1wEJvtp+IBraswOTB8Xx3JpMKqpt+A1jgtHximoWrdvP1BE96NO1nes4Z8XlNYjvAu80skyB90Vkk4jM8WMmn5tzUX/ySyp4c+sB11GMMT6wdGM2xeWtb1gNb5wUCBH5T6AaWNTIKheq6ljgKuBHInLRKd5rjoikiEhKfn6+D9K2rAsHxnFOj1jmr8pA1YbfMCaYVNfU8uyn+xjftzNjkjq7jnPW/F4gROR24BpgtjbyDamquZ4/84DXgQmNvZ+qzlPVZFVNjo+P90HiliUizLmoP3vyjvPPtMAvaMaYpnsn9RC5RSeC4ugB/FwgRGQq8HNgmqp67RAgIu1EJPbkNHAlkOpt3dbqmpG96NEhmvl232pjgsbJYTX6xbXjiqHdXcdpEb5s5voisBYYIiI5InIn8CQQC6z0NGF92rNuLxFZ4Xlpd+BTEdkGbACWq+q7vsrpQlRE3fAba/YWkJp7zHUcY0wLWL+vkB25x/heKx1Ww5sIX72xqs7yMvvZRtY9AFztmc4ARvkqV6CYNTGJv3yUzoLVGfx55hjXcYwxZ2nB6gy6tIvihrHB0zjTelI70iE6kpnjE3lr+0Eyj5S6jmOMOQs7DxTzwa48vnNe6x1WwxsrEA7Nubg/keHC4x/ucR3FGHMW5q5MIzY6gjvOb90d4xqyAuFQt9hobjuvL29szSU9r8R1HGNMM2zLLuKDXYeZM7k/HWMiXcdpUVYgHPv+xQOIiQznsQ/sKMKY1mjuyjQ6x0RyRysfVsMbKxCOdWkXxR0X9GP59oPsPFDsOo4x5gykZBbySVo+3794AO3b+KzNjzNWIALAXZP7ExsdwWMfpLmOYow5A396P4249m34znmtd0jvU7ECEQA6xkRy1+T+rNx5mO05dltSY1qDNelHWJtRwA8vGUBMVPAdPYAViIBxxwV96RQTyZ/et6MIYwKdqvKnlWn06BDNLROTXMfxGSsQASI2OpK7Lx7AJ2n5pGQWnv4FxhhnPknLZ9P+o/z4soFB1e+hISsQAeQ75/Uhrn2UHUUYE8BUlbkr00jo3JabkxNdx/EpKxABJCYqgh9eMpC1GQWs2XvEdRxjjBd11wqP8ZPLBxEVEdxfocH9t2uFbpmYRI8O0cx9P83uF2FMgKmtrTt66BfXjuvH9HYdx+esQASY6MhwfnTZQFL2H+UTu1+EMQFlRepBdh8q4aeXDyIiPPi/PoP/b9gKzUhOpHentsxdaUcRxgSKmlrlsZVpDOrWnmtH9XIdxy+sQASgqIgwfnr5ILbnHGPlzsOu4xhjgDe35rI3v5R7pwwmPEju93A6ViAC1PVje9O3awxzV6ZRW2tHEca4VFVTy+Mf7mFozw5MHd7DdRy/sQIRoCLCw7jnisHsPlTCO6mHXMcxJqS9tjmH/QVl3D9lcNDcLa4prEAEsGtH9WJQt/Y89kEaNXYUYYwTFdU1PPFhOqMSO3H50G6u4/iVTwuEiCwUkTwRSa03r4uIrBSRPZ4/Ozfy2ts86+wRkdt8mTNQhYcJ904ZTHrecZZty3Udx5iQtHRjNrlFJ7h/ymBEQufoAXx/BPEcMLXBvAeAD1V1EPCh5/lXiEgX4CFgIjABeKixQhLspg7vwdCeHXj8gz1U1dS6jmNMSCmvquHJj9MZ37czkwfFuY7jd00qECLSTkTCPNODRWSaiJz21kmqugpoOLDQdOB5z/TzwHVeXvoNYKWqFqrqUWAlXy80ISEsTLhvymAyC8p4bXOO6zjGhJR/rNvP4eIK7psyJOSOHqDpRxCrgGgR6Q28D9xK3dFBc3RX1YOe6UNAdy/r9Aay6z3P8cz7GhGZIyIpIpKSnx+cHcuuGNqNUQkdeeLDdCqqa1zHMSYklFZU8/Qne7lgYFfOG9DVdRwnmlogRFXLgOuB/1PVm4DhZ7txresFdlZXX1V1nqomq2pyfHz82UYKSCLCfVcOIbfoBEs3Zp/+BcaYs/b82kyOHK/kvilDXEdxpskFQkTOA2YDyz3zmjvG7WER6el5055Anpd1coH6wyQmeOaFrIsGxZHcpzNPfpxOeZUdRRjjSyXlVcxblcElQ+IZ1yckL38CTS8Q9wAPAq+r6uci0h/4uJnbXAacbJV0G/Cml3XeA64Ukc6ei9NXeuaFLBHh/iuHcLi4gkXrs1zHMSaoLfw0k6KyKu4P4aMHaGKBUNVPVHWaqv7ec7H6iKr+5HSvE5EXgbXAEBHJEZE7gd8BU0RkD3CF5zkikiwiCzzbKwR+A2z0PB7xzAtp5w3oyvkDuvLUP9Mpq6x2HceYoFRUVsmC1RlcOaw75yZ0dB3Hqaa2YlosIh1EpB2QCuwUkZ+d7nWqOktVe6pqpKomqOqzqlqgqper6iBVveLkF7+qpqjq9+q9dqGqDvQ8/tbcv2Cwuf/KwRw5Xsnza/a7jmJMUJq/OoOSimrunTLYdRTnmnqKaZiqFlPXJPUdoB91LZmMn43r04VLhsTzzKq9lJRXuY5jTFApOF7B3z7L5JqRPRnas4PrOM41tUBEevo9XAcsU9UqzrL1kWm++6YMpqisioWfZrqOYkxQefqTvZRX1XDPFXb0AE0vEM8AmUA7YJWI9AGKfRXKnNrIhE5cOaw7C1ZncLS00nUcY4LC4eJyXli7n+vG9GZgt/au4wSEpl6kfkJVe6vq1VpnP3Cpj7OZU7j/yiGUVlYzd2Wa6yjGBIXfvbMbVbjncjt6OKmpF6k7isjckz2WReRP1B1NGEeG9Ijl1kl9WLR+P58fOOY6jjGt2sbMQl7fkstdF/UjqWuM6zgBo6mnmBYCJcDNnkcxYC2LHLtvyhA6xUTx0Juf261JjWmmmlrlV29+Ts+O0fzo0oGu4wSUphaIAar6kKpmeB6/Bvr7Mpg5vY4xkfzH1CGk7D/KG1tDuqO5Mc22eP1+dh0s5r++OYyYqAjXcQJKUwvECRG58OQTEbkAOOGbSOZM3DQukVEJHXl0xW5r9mrMGSosreSP76dxXv+uXH1u6NxKtKmaWiDuBv4qIpkikgk8CXzfZ6lMk4WFCb+ePoL8kgr+8lG66zjGtCp/eO8LjldU8+vpw0NyOO/TaWorpm2qOgoYCYxU1THAZT5NZppsdGInbk5OYOGn+0jPO+46jjGtwo6cY7y0MYvbzuvL4O6xruMEpDO6o5yqFnt6VAPc54M8ppl+PvUc2kaF8/Ayu2BtzOnU1iq/WpZK13ZR3DNlkOs4Aetsbjlqx2MBJK59G+6fMphP04/w3ueHXMcxJqC9ujmHLVlFPHDVUDpEn/bmmCHrbAqE/UwNMN+e1IdzesTym7d3caLS7hlhjDfF5VX8/t3djEnqxPVjvN6o0nicskCISImIFHt5lAC9/JTRNFFEeBgPTxtObtEJnvpkr+s4xgSkP6/cQ0FpJY9MG0FYmJ0IOZVTFghVjVXVDl4esapqDYYD0KT+XZk2qhdPf7KXrIIy13GMCShph0t4fm0msyYkhfy9HpribE4xmQD1i6uHEhEm/Gb5TtdRjAkYqspDb35O+zYR/OzK0L5TXFNZgQhCPTpG82+XDWLlzsP88wtvt/w2JvQs33GQtRkF/Ps3htC5XZTrOK2C3wuEiAwRka31HsUick+DdS4RkWP11vmVv3O2dt+9sC/94trx67d2UlFtF6xNaCurrOa/l+9iWM8O3DIhyXWcVsPvBUJVv1DV0ao6GhgHlAGve1l19cn1VPUR/6Zs/dpEhPPQtcPYd6TUbixkQt5fP07n4LFyHpk+nHC7MN1krk8xXQ7s9dxfwrSwS4Z0Y8qw7vzloz0cOlbuOo4xTmQeKWX+qn1cP6Y3yX27uI7TqrguEDOBFxtZdp6IbBORd0RkeGNvICJzTt6nIj8/3zcpW7FffnMY1bXKoyt2uY5ijBOPvL2TqIgwHrjqHNdRWh1nBUJEooBpwMteFm8G+njGf/oL8EZj76Oq81Q1WVWT4+PjfRO2FUvqGsPdFw9g2bYDrMsocB3HGL/6cNdhPtqdx08vH0S3DtGu47Q6Lo8grgI2q+rhhgs8Yz4d90yvACJFJM7fAYPFDy4eQO9ObXl42edU19S6jmOMX5RX1fDI2zsZ2K09t1/Q13WcVsllgZhFI6eXRKSHeMbeFZEJ1OW0n7/N1DYqnF9eM5Tdh0r4xzq73GNCw4LVGewvKOPha4cTGe76bHrr5GSviUg7YArwWr15d4vI3Z6nNwKpIrINeAKYqTZE6Vn5xvAeTB4Ux9yVaRw5XuE6jjE+lVt0gic/TueqET24cJCdfGguJwVCVUtVtauqHqs372lVfdoz/aSqDlfVUao6SVXXuMgZTESEh64dTlllDX949wvXcYzxqUeX1zXK+M9vDnWcpHWz464QMrBbe757YT+WpGSzNbvIdRxjfOKz9CMs33GQH14ykITOMa7jtGpWIELMv102kG6xbXjozVRqa+2snQkuVTW1PLzsc5K6xDDnov6u47R6ViBCTGx0JA9efQ7bco6xaEOW6zjGtKgFq/exJ+84v7xmGNGR4a7jtHpWIELQdaN7M3lQHI8u30VGvt3D2gSHnQeKmbvyC6YO78EVQ7u5jhMUrECEIBHhDzeOIioijHuXbKXK+kaYVq68qoZ7lmyhU0wUj15/Lp5W8uYsWYEIUT06RvPot85lW84x/vJRuus4xpyV/333C9IOH+cPN46kiw3l3WKsQISwb47syfVjevPXj9PZnHXUdRxjmuXTPUdY+Nk+vnNeHy4ZYqeWWpIViBD38PTh9OgQzb1LtlJaUe06jjFnpKiskn9/eRsD4tvx4FXW56GlWYEIcR2iI3lsxmiyCsv4rd2i1LQiqsp/vZHKkeMVPD5zDG2jrNVSS7MCYZjQrwvfv2gAL27IZuXOr42daExAenPrAd7efpB7pwxmRO+OruMEJSsQBoD7pgxmWM8OPPDqdvJLbKwmE9hyjpbxyzdTSe7TmbsvHuA6TtCyAmEAiIoI488zR1NSUc1/vLodGxvRBKqaWuX+pduorVUemzHabiHqQ1YgzJcGd4/lgann8NHuPBZbL2sToBaszmD9vkIemjacxC421pIvWYEwX3H7+X25cGAcv33belmbwLPzQDF/fP8LvjG8OzeNS3AdJ+hZgTBfERYm/PEmTy/rpdusl7UJGPV7S//P9SOtt7QfWIEwX/NlL+vsIp60XtYmQPzhPest7W9WIIxXJ3tZP/lxOlusl7Vx7LP0Izz7qfWW9jcrEKZR1svaBIJjZVXcv9R6S7vgrECISKaI7BCRrSKS4mW5iMgTIpIuIttFZKyLnKGsQ3Qkc28exX7rZW0cUVX+840dHDlewZ9nWG9pf3N9BHGpqo5W1WQvy64CBnkec4Cn/JrMADCxf1fmXNTfelkbJ072lr7nikGcm2C9pf3NdYE4lenAC1pnHdBJRHq6DhWK7psymKHWy9r4WW7RCX75ZirjrLe0My4LhALvi8gmEZnjZXlvILve8xzPvK8QkTkikiIiKfn5+T6KGtraRITzuKeX9QPWy9r4QW2tcv/SrXW9pW8eTUR4IP+WDV4u9/qFqjqWulNJPxKRi5rzJqo6T1WTVTU5Pj6+ZROaL53sZf3h7jxe3JB9+hcYcxYWfJrBuoy63tJJXa23tCvOCoSq5nr+zANeByY0WCUXSKz3PMEzzzhyspf1b97eyb4jpa7jmCC180Axf3wvzXpLBwAnBUJE2olI7Mlp4EogtcFqy4DveFozTQKOqepBP0c19dTvZf2Df2yipLzKdSQTZI6WVvKjxZvpGBNpvaUDgKsjiO7ApyKyDdgALFfVd0XkbhG527POCiADSAfmAz90E9XU16NjNE/eMoY9ecf58eItVNtQHKaFVFTX8P2/byK36ARPf3us9ZYOABJMFxyTk5M1JeVrXSqMD7y4IYsHX9vBrZP68Mj04fZLz5wVVeW+pdt4fUsuT8waw7RRvVxHChkisqmRrgZE+DuMCQ6zJiSReaSUZ1Zl0DeuHXde2M91JNOKPfFhOq9vyeX+KYOtOAQQKxCm2f5j6jnsL6jrZZ3UJYYpw7q7jmRaoTe35vLYB2ncMDaBH1820HUcU481LjbNFhYmPDZjNCN7d+QnL24hNfeY60imldmYWcjPXt7OxH5d+J/rz7VTlQHGCoQ5K22jwpl/WzJd2kVx5/MbOXjshOtIppXIPFLKnBdSSOjclmduHUdUhH0dBRr7FzFnrVtsNAtvH09pRQ3ffS6F4zbyqzmNorJKvvvcRgAW3j6eTjHWYikQWYEwLWJIj1j+OnssaYdL+MmL1vzVNK6yupa7/7GJnKMnmPedZPrGtXMdyTTCCoRpMRcPjufX04bz0e48frt8l+s4JgCpKg++toN1GYX8740jGd+3i+tI5hSsFZNpUd+e1IfMI6Us+HQffbvGcPsF1vzV/MtfP07n1c053HPFIK4b87WxN02AsQJhWtyDVw9lf2EZj7y9k6SuMVx2jjV/NfDWtgP88f00vjWmNz+9fJDrOKYJ7BSTaXHhYcLjM0czrFcHfrx4C58fsOavoW7T/kLuf3kb4/t25nc3WHPW1sIKhPGJmKgInr1tPB3bRnLncykcOlbuOpJxJKugjLte2ETPjtE8c2sybSLstqGthRUI4zPdO0Tz7G3jKSmv4s7nN1JqzV9DzrGyKu54bgM1tcrfbh9vA/C1MlYgjE8N69WBJ28Zy66Dxfz0pS3U1AbP4JDm1Cqra/nBok1kFZbxzK3j6B/f3nUkc4asQBifu/Scbjx07XA+2JXHf1vz15CgqvzXGztYs7eA310/kkn9u7qOZJrBWv/ZJXkAAA6ySURBVDEZv7jt/L7sO1LKws/20adrDLed39d1JOND//fPvSxNyeHfLhvIDXZXuFbLCoTxm19eM4ycoyd4aNnnVFTXMOeiAa4jmRamqjz2wR6e+HAP00b14r4pg11HMmfBTjEZvwkPE/46ewzfHNmTR1fs5r+X76TWrkkEjZpa5T/fSOWJD/dwc3ICc28eZc1ZWzm/H0GISCLwAnW3HVVgnqo+3mCdS4A3gX2eWa+p6iP+zGl8o01EOH+ZOYa4dlHMX72PguOV/P7GkUSG22+V1qy8qoZ7XtrKu58f4oeXDOBn3xhixSEIuDjFVA3cr6qbRSQW2CQiK1V1Z4P1VqvqNQ7yGR8LCxMenjacuPZt+NPKNI6WVfLX2WOJibIznq1RcXkVc15IYV1GIb+6ZhjftbsLBg2//2xT1YOqutkzXQLsAmxQlhAjIvzb5YP4n+vP5ZO0fGYvWM/R0krXscwZyispZ+Yz60jJPMrjM0dbcQgyTo/rRaQvMAZY72XxeSKyTUTeEZHhp3iPOSKSIiIp+fn5PkpqfGXWhCT+b/Y4Pj9QzE3PrOVAkd1wqLXYX1DKjU+tJbOglGdvH8/00fY7L9g4KxAi0h54FbhHVYsbLN4M9FHVUcBfgDcaex9VnaeqyaqaHB8f77vAxmemjujBC9+dwOFj5dzw1Br2HC5xHcmcRmruMW54ag0l5VUsvmsSFw+2z14wclIgRCSSuuKwSFVfa7hcVYtV9bhnegUQKSJxfo5p/GhS/64s+f55VNcqNz2zlk37j7qOZBqxJv0IM+eto01EOK/84HxGJ3ZyHcn4iN8LhNQ1bXgW2KWqcxtZp4dnPURkAnU5C/yX0rgwrFcHXvvB+XRqG8nsBev4aPdh15FMAyt2HOT2v22kd6e2vPqD8xlgw2cENRdHEBcAtwKXichWz+NqEblbRO72rHMjkCoi24AngJmqag3mQ0Bilxhe+cH5DOoWy10vbOLVTTmuIxmPv6/bz48Wb2ZkQkeWfv88enSMdh3J+JgE0/ducnKypqSkuI5hWsDximq+//cUPksv4BdXn2O9rh1SVf78wR4e/3APVwztxpO3jCU60obsDhYisklVk70ts95JJiC1bxPBwtvHf9nr+tEVu6zXtQM1tcp/vZHK457e0U9/e5wVhxBiPZNMwKrf63reqgyOHK/g9zdYr2t/Ka+q4d4lW3kn1XpHhyorECagNex1XXC8krk3j6Jr+zauowW1w8Xl/PSlLdY7OsTZTzET8Or3ul67t4Ar5n7Cm1tzCabrZ4FCVVmyMYsr5n7Clqwi6x0d4uwIwrQasyYkkdynMz9/dTs/fWkrb249wG+vG0GvTm1dRwsKWQVlPPDadtbsLWBivy78/oaR9I1r5zqWcchaMZlWp6ZWeX5NJn947wvCw4QHrjqHWyYkERZm58ebo6ZWeW5NJn/07M9fXD2UmeMTbX+GiFO1YrICYVqtrIIyHnx9O5+l1/3i/d0NI+lnv3jPSNrhEn7+yna2Zhdx2Tnd+O9vjaBnRzsiCyVWIEzQUlVeTsnhN8t3Ulldy31TBnPnhf2IsJZOp1RZXctT/9zLkx/vITY6koeuHca0Ub2slVIIOlWBsGsQplUTEW4en8jFQ+L55Rup/M87u1m+4yC/v2EkQ3t2cB0vIG3LLuI/Xt3O7kMlTB/di19dM8xahRmv7AjCBA1VZcWOQzy0LJWisip+eMkAfnTZQNpEWMcugBOVNTz2QRoLVmfQLTaa3143giuGdXcdyzhmRxAmJIgI3xzZk/MHdOU3y3fyxEfpvJN6iN/fOJKxSZ1dx3Nq7d4CHnxtO5kFZdwyMYkHrjqHDtGRrmOZAGcnak3Q6dwuirk3j+Zvd4yntKKaG55awyNv7aSsstp1NL8rLq/iF6/vYNb8dSiw+K6JPPqtc604mCaxU0wmqB2vqOZ/393NC2v3k9C5LXdN7s91o3vTMSa4vyALSyt5bXMOC1bvI6+knO9N7s+9VwymbZSdbjNfZa2YTMjbsK+Q3y7fyfacY0RFhHH1iB7cPD6RSf26Bk17/9pa5dP0IyzZmM37Ow9RVaOMTerEr64dbjf1MY2yAmGMR2ruMZamZPPGllyKy6vp0zWGm5MTuXFcAt07tM77GxwoOsHLKTksTckmt+gEnWMiuX5sAjPGJzK4e6zreCbAWYEwpoHyqhreTT3ESxuzWJdRSHiYcOmQeGaMT+LSIfEB34+isrqWD3cd5qWN2azakw/AhQPjmDE+kSnDulvLLdNkViCMOYXMI6UsTcnm5U055JdU0C22DTeMS2BGcmLAjUWUnlfCko3ZvLY5l4LSSnp2jOam5ERuGpdAYpcY1/FMK2QFwpgmqK6p5eMv8lmyMYuPv8inplaZ1L8LM8cnMXVED2c3yimrrGb59oMs2ZhNyv6jRIQJU4Z1Z8b4RCYPiic8SK6hGDcCrkCIyFTgcSAcWKCqv2uwvA3wAjAOKABmqGrm6d7XCoRpKYeLy3llU915/f0FZXSIjmBUYicSu8SQ2DmGxC5tSfJMd4qJPOshKlSVwtJKso+eILuwjOyjZXV/Fp5ga3YRxyuqGRDfjhnjE7l+bAJx1vPZtJCAKhAiEg6kAVOAHGAjMEtVd9Zb54fASFW9W0RmAt9S1Rmne28rEKal1dYq6/YV8NrmXNIOl5BdWMbRsqqvrNO+TQQJnT0Fo0sMiZ3bktS1rngkdI75smlpWWU12YX/KgBZngKQ4ykGpZU1X3nfru2iSOgSw9Aesdw4LoFxfTrbWEmmxQVaT+oJQLqqZgCIyEvAdGBnvXWmAw97pl8BnhQR0WA6H2ZahbAw4fwBcZw/IO7LeSXlVXVf9J4v9pyjJ8gqLGPfkVJW7cmnvKr2K+9R92tfOXK88ivzY6LCvzwamdS/678KTJe2JHaOoV0bG+jAuOXif2BvILve8xxgYmPrqGq1iBwDugJHGr6ZiMwB5gAkJSX5Iq8xXxEbHcmwXpEM6/X1wQBV6wpBVmHZl0cG2YUnEMHz5V93hJHYJYau7aLsiMAEtFb/E0VV5wHzoO4Uk+M4JsSJCPGxbYiPbcO4PqE9/pNp/Vw09s4FEus9T/DM87qOiEQAHam7WG2MMcZPXBSIjcAgEeknIlHATGBZg3WWAbd5pm8EPrLrD8YY419+P8XkuabwY+A96pq5LlTVz0XkESBFVZcBzwJ/F5F0oJC6ImKMMcaPnFyDUNUVwIoG835Vb7ocuMnfuYwxxvxLYA84Y4wxxhkrEMYYY7yyAmGMMcYrKxDGGGO8CqrRXEUkH9jfzJfH4aWndgCwXGfGcp0Zy3VmgjFXH1WN97YgqArE2RCRlMYGrHLJcp0Zy3VmLNeZCbVcdorJGGOMV1YgjDHGeGUF4l/muQ7QCMt1ZizXmbFcZyakctk1CGOMMV7ZEYQxxhivrEAYY4zxKuQKhIhMFZEvRCRdRB7wsryNiCzxLF8vIn0DJNftIpIvIls9j+/5IdNCEckTkdRGlouIPOHJvF1Exvo6UxNzXSIix+rtq195W88HuRJF5GMR2Skin4vIT72s4/d91sRcft9nIhItIhtEZJsn16+9rOP3z2MTc/n981hv2+EiskVE3vayrGX3l6qGzIO64cX3Av2BKGAbMKzBOj8EnvZMzwSWBEiu24En/by/LgLGAqmNLL8aeAcQYBKwPkByXQK87eD/V09grGc6Fkjz8u/o933WxFx+32eefdDeMx0JrAcmNVjHxeexKbn8/nmst+37gMXe/r1aen+F2hHEBCBdVTNUtRJ4CZjeYJ3pwPOe6VeAy8X3Nw5uSi6/U9VV1N2PozHTgRe0zjqgk4j0DIBcTqjqQVXd7JkuAXZRd3/1+vy+z5qYy+88++C452mk59Gw1YzfP49NzOWEiCQA3wQWNLJKi+6vUCsQvYHses9z+PoH5ct1VLUaOAZ0DYBcADd4Tku8IiKJXpb7W1Nzu3Ce5xTBOyIy3N8b9xzaj6Hu12d9TvfZKXKBg33mOV2yFcgDVqpqo/vLj5/HpuQCN5/HPwM/B2obWd6i+yvUCkRr9hbQV1VHAiv5168E83WbqRtfZhTwF+ANf25cRNoDrwL3qGqxP7d9KqfJ5WSfqWqNqo6m7t70E0RkhD+2ezpNyOX3z6OIXAPkqeomX2/rpFArELlA/Uqf4JnndR0RiQA6AgWuc6lqgapWeJ4uAMb5OFNTNGV/+p2qFp88RaB1dy+MFJE4f2xbRCKp+xJepKqveVnFyT47XS6X+8yzzSLgY2Bqg0UuPo+nzeXo83gBME1EMqk7DX2ZiPyjwTotur9CrUBsBAaJSD8RiaLuIs6yBussA27zTN8IfKSeKz4uczU4Tz2NuvPIri0DvuNpmTMJOKaqB12HEpEeJ8+7isgE6v6f+/xLxbPNZ4Fdqjq3kdX8vs+aksvFPhOReBHp5JluC0wBdjdYze+fx6bkcvF5VNUHVTVBVftS9x3xkap+u8FqLbq/nNyT2hVVrRaRHwPvUddyaKGqfi4ijwApqrqMug/S30UknboLoTMDJNdPRGQaUO3Jdbuvc4nIi9S1bokTkRzgIeou2KGqT1N3X/GrgXSgDLjD15mamOtG4AciUg2cAGb6ochD3S+8W4EdnvPXAL8Akuplc7HPmpLLxT7rCTwvIuHUFaSlqvq2689jE3P5/fPYGF/uLxtqwxhjjFehdorJGGNME1mBMMYY45UVCGOMMV5ZgTDGGOOVFQhjjDFeWYEwxhjjlRUIY4wxXlmBMMYHRORnIvITz/RjIvKRZ/oyEVnkNp0xTWMFwhjfWA1M9kwnA+094yFNBlY5S2XMGbACYYxvbALGiUgHoAJYS12hmExd8TAm4IXUWEzG+IuqVonIPurG6FkDbAcuBQYSGAMtGnNadgRhjO+sBv6dulNKq4G7gS1+GjjQmLNmBcIY31lN3ciga1X1MFCOnV4yrYiN5mqMMcYrO4IwxhjjlRUIY4wxXlmBMMYY45UVCGOMMV5ZgTDGGOOVFQhjjDFeWYEwxhjj1f8DETj7c9qhyusAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElOENGw3x1x"
      },
      "source": [
        "## Manual Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsBTaI4q3x2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34f5056a-6282-458c-feab-e7f1ac005581"
      },
      "source": [
        "# random guss w\n",
        "w = 0.5\n",
        "\n",
        "# y = 2*x + 1, so x = 4 should return 8\n",
        "# Before training\n",
        "print(\"(before) forward(4): \", forward(4))\n",
        "print()\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(15):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # Compute derivative w.r.t to the learned weights\n",
        "        # Update the weights\n",
        "        # Compute the loss and print progress\n",
        "        grad = gradient(x_val, y_val)\n",
        "        w = w - 0.01 * grad  # update w\n",
        "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
        "        l = loss(x_val, y_val)\n",
        "    print(\"epoch:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2), '\\n')\n",
        "\n",
        "# After training\n",
        "print(\"after: \", forward(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(before) forward(4):  2.0\n",
            "\n",
            "\tgrad:  1.0 2.0 -3.0\n",
            "\tgrad:  2.0 4.0 -11.76\n",
            "\tgrad:  3.0 6.0 -24.34\n",
            "epoch: 0 w= 0.89 loss= 11.07 \n",
            "\n",
            "\tgrad:  1.0 2.0 -2.22\n",
            "\tgrad:  2.0 4.0 -8.69\n",
            "\tgrad:  3.0 6.0 -18.0\n",
            "epoch: 1 w= 1.18 loss= 6.05 \n",
            "\n",
            "\tgrad:  1.0 2.0 -1.64\n",
            "\tgrad:  2.0 4.0 -6.43\n",
            "\tgrad:  3.0 6.0 -13.31\n",
            "epoch: 2 w= 1.39 loss= 3.31 \n",
            "\n",
            "\tgrad:  1.0 2.0 -1.21\n",
            "\tgrad:  2.0 4.0 -4.75\n",
            "\tgrad:  3.0 6.0 -9.84\n",
            "epoch: 3 w= 1.55 loss= 1.81 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.9\n",
            "\tgrad:  2.0 4.0 -3.51\n",
            "\tgrad:  3.0 6.0 -7.27\n",
            "epoch: 4 w= 1.67 loss= 0.99 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.66\n",
            "\tgrad:  2.0 4.0 -2.6\n",
            "\tgrad:  3.0 6.0 -5.38\n",
            "epoch: 5 w= 1.76 loss= 0.54 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.49\n",
            "\tgrad:  2.0 4.0 -1.92\n",
            "\tgrad:  3.0 6.0 -3.98\n",
            "epoch: 6 w= 1.82 loss= 0.3 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.36\n",
            "\tgrad:  2.0 4.0 -1.42\n",
            "\tgrad:  3.0 6.0 -2.94\n",
            "epoch: 7 w= 1.87 loss= 0.16 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.27\n",
            "\tgrad:  2.0 4.0 -1.05\n",
            "\tgrad:  3.0 6.0 -2.17\n",
            "epoch: 8 w= 1.9 loss= 0.09 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.2\n",
            "\tgrad:  2.0 4.0 -0.78\n",
            "\tgrad:  3.0 6.0 -1.61\n",
            "epoch: 9 w= 1.93 loss= 0.05 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.15\n",
            "\tgrad:  2.0 4.0 -0.57\n",
            "\tgrad:  3.0 6.0 -1.19\n",
            "epoch: 10 w= 1.95 loss= 0.03 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.11\n",
            "\tgrad:  2.0 4.0 -0.42\n",
            "\tgrad:  3.0 6.0 -0.88\n",
            "epoch: 11 w= 1.96 loss= 0.01 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.08\n",
            "\tgrad:  2.0 4.0 -0.31\n",
            "\tgrad:  3.0 6.0 -0.65\n",
            "epoch: 12 w= 1.97 loss= 0.01 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.06\n",
            "\tgrad:  2.0 4.0 -0.23\n",
            "\tgrad:  3.0 6.0 -0.48\n",
            "epoch: 13 w= 1.98 loss= 0.0 \n",
            "\n",
            "\tgrad:  1.0 2.0 -0.04\n",
            "\tgrad:  2.0 4.0 -0.17\n",
            "\tgrad:  3.0 6.0 -0.35\n",
            "epoch: 14 w= 1.98 loss= 0.0 \n",
            "\n",
            "after:  7.935350141047644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKbVM9Oqn9nY"
      },
      "source": [
        "# Auto Gradient Version\n",
        "\n",
        "We use different loss function!\n",
        "\n",
        "可能有一些requires_grad的原因在里面"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpkAKz2m_gd5"
      },
      "source": [
        "import torch\n",
        "\n",
        "def loss_new(y_pred, y_val):\n",
        "    return (y_pred - y_val)**2\n",
        "\n",
        "# # 'Tensor' object is not callable\n",
        "# def loss_new(x, y_val):\n",
        "#     return (x * w - y_val)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cbbPZKnn9nb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3079b49e-2c47-4179-d908-8c4eced9e2ca"
      },
      "source": [
        "# random guss w\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# Before training\n",
        "print(\"(before) forward(4): \", forward(4))\n",
        "print()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(15):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred = forward(x_val)  # 1) Forward pass\n",
        "        # l = loss(x_val, y_val)  # => 'Tensor' object is not callable\n",
        "        l = loss_new(y_pred, y_val)  # 2) Compute loss\n",
        "        l.backward()  # 3) Back propagation to update weights\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item()  # update w\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "    print(\"epoch:\", epoch, \"w=\", w.data, \"loss=\", l.item(), '\\n')\n",
        "\n",
        "# After training\n",
        "print(\"after: \", forward(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(before) forward(4):  tensor([4.], grad_fn=<MulBackward0>)\n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -2.0\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -7.840000152587891\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -16.228801727294922\n",
            "epoch: 0 w= tensor([1.2607]) loss= 7.315943717956543 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -1.478623867034912\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -5.796205520629883\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -11.998146057128906\n",
            "epoch: 1 w= tensor([1.4534]) loss= 3.9987640380859375 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -1.0931644439697266\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -4.285204887390137\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -8.870372772216797\n",
            "epoch: 2 w= tensor([1.5959]) loss= 2.1856532096862793 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.8081896305084229\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -3.1681032180786133\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -6.557973861694336\n",
            "epoch: 3 w= tensor([1.7012]) loss= 1.1946394443511963 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.5975041389465332\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -2.3422164916992188\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -4.848389625549316\n",
            "epoch: 4 w= tensor([1.7791]) loss= 0.6529689431190491 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.4417421817779541\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -1.7316293716430664\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -3.58447265625\n",
            "epoch: 5 w= tensor([1.8367]) loss= 0.35690122842788696 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.3265852928161621\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -1.2802143096923828\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -2.650045394897461\n",
            "epoch: 6 w= tensor([1.8793]) loss= 0.195076122879982 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.24144840240478516\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.9464778900146484\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -1.9592113494873047\n",
            "epoch: 7 w= tensor([1.9107]) loss= 0.10662525147199631 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.17850565910339355\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.699742317199707\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -1.4484672546386719\n",
            "epoch: 8 w= tensor([1.9340]) loss= 0.0582793727517128 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.1319713592529297\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.5173273086547852\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -1.070866584777832\n",
            "epoch: 9 w= tensor([1.9512]) loss= 0.03185431286692619 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.09756779670715332\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.3824653625488281\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -0.7917022705078125\n",
            "epoch: 10 w= tensor([1.9639]) loss= 0.017410902306437492 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.07213282585144043\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.2827606201171875\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -0.5853137969970703\n",
            "epoch: 11 w= tensor([1.9733]) loss= 0.009516451507806778 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.053328514099121094\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.2090473175048828\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -0.43272972106933594\n",
            "epoch: 12 w= tensor([1.9803]) loss= 0.005201528314501047 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.039426326751708984\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.15455150604248047\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -0.3199195861816406\n",
            "epoch: 13 w= tensor([1.9854]) loss= 0.0028430151287466288 \n",
            "\n",
            "\tgrad:  tensor([1.]) tensor([2.]) -0.029148340225219727\n",
            "\tgrad:  tensor([2.]) tensor([4.]) -0.11426162719726562\n",
            "\tgrad:  tensor([3.]) tensor([6.]) -0.23652076721191406\n",
            "epoch: 14 w= tensor([1.9892]) loss= 0.0015539465239271522 \n",
            "\n",
            "after:  tensor([7.9569], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojiz86Nan9nm"
      },
      "source": [
        "# Linear Model (Torch)\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/nn_tutorial.html?highlight=zero_grad\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step\n",
        "\n",
        "https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "\n",
        "\n",
        "We need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Rp5JXGn9no"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zofyQhjn9nv"
      },
      "source": [
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn4AB1vhn9nz"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c-Jkm2Sn9n4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "5b3d67c6-88a8-4ae6-a80b-de04bd6a5c93"
      },
      "source": [
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # learn rate\n",
        "\n",
        "# Training loop (forward + backward)\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "    # print('Data: {}, Predict: {}'.format(y_data, y_pred))\n",
        "    \n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a [[backward]] pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 111.7068862915039 \n",
            "Epoch: 50 | Loss: 0.18679173290729523 \n",
            "Epoch: 100 | Loss: 0.09057698398828506 \n",
            "Epoch: 150 | Loss: 0.043921686708927155 \n",
            "Epoch: 200 | Loss: 0.021298028528690338 \n",
            "Epoch: 250 | Loss: 0.010327618569135666 \n",
            "Epoch: 300 | Loss: 0.005007944535464048 \n",
            "Epoch: 350 | Loss: 0.0024283973034471273 \n",
            "Epoch: 400 | Loss: 0.001177570316940546 \n",
            "Epoch: 450 | Loss: 0.0005710131372325122 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvSZHJXIn9n_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3e054ae-35f9-4ed5-cef1-862894eb54af"
      },
      "source": [
        "# After training\n",
        "hour_var = tensor([[4.0]])  # create var\n",
        "y_pred = model(hour_var)  # forward\n",
        "print(\"Prediction (after training)\",  4, y_pred.data[0][0].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction (after training) 4 7.980732440948486\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"0x04 Softmax and more.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false,"id":"riOXP77XNij1","colab_type":"text"},"source":["# Softmax"]},{"cell_type":"markdown","metadata":{"id":"XgMc9rMINij4","colab_type":"text"},"source":["## loss"]},{"cell_type":"code","metadata":{"id":"01FXv5jvNij5","colab_type":"code","colab":{}},"source":["from torch import nn, tensor, max\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dn_ggYDNij-","colab_type":"code","colab":{},"outputId":"75d2f6d2-a974-4dbf-cf74-662cd2f83112"},"source":["# Cross entropy example (np only)\n","# One hot\n","# 0: 1 0 0\n","# 1: 0 1 0\n","# 2: 0 0 1\n","Y = np.array([1, 0, 0])\n","Y_pred1 = np.array([0.7, 0.2, 0.1])  # small loss\n","Y_pred2 = np.array([0.1, 0.3, 0.6])  # large loss\n","\n","# calculate loss manually\n","print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n","print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss1: 0.3567\n","Loss2: 2.3026\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YHjvr8m6NikE","colab_type":"code","colab":{}},"source":["# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n","loss = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6h1ZKsm4NikJ","colab_type":"code","colab":{},"outputId":"7791d605-e0aa-4198-bb99-051675344c57"},"source":["# target is of size nBatch\n","# each element in target has to have 0 <= value < nClasses (0-2)\n","# Input is class, not one-hot!!!\n","Y = tensor([0], requires_grad=False)  # means class 0 is correct\n","\n","# input is of size nBatch x nClasses = 1 x 4\n","# Y_pred are logits (not softmax)!!!\n","Y_pred1 = tensor([[2.0, 1.0, 0.1]])  # should be small, since [0] is larger than the rest\n","Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n","\n","l1 = loss(Y_pred1, Y)\n","l2 = loss(Y_pred2, Y)\n","\n","print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n","print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n","print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch Loss1: 0.4170 \n","PyTorch Loss2: 1.8406\n","Y_pred1: 0\n","Y_pred2: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-MAr_Ks-NikN","colab_type":"code","colab":{},"outputId":"308cc91c-d0eb-4f39-8690-fd1676cbaea2"},"source":["# target is of size nBatch\n","# each element in target has to have 0 <= value < nClasses (0-2)\n","# Input is class, not one-hot\n","Y = tensor([2, 0, 1], requires_grad=False)\n","\n","# input is of size nBatch x nClasses = 2 x 4\n","# Y_pred are logits (not softmax)\n","Y_pred1 = tensor([[0.1, 0.2, 0.9],\n","                  [1.1, 0.1, 0.2],\n","                  [0.2, 2.1, 0.1]])  # should be small\n","\n","Y_pred2 = tensor([[0.8, 0.2, 0.3],\n","                  [0.2, 0.3, 0.5],\n","                  [0.2, 0.2, 0.5]])  # should be large\n","\n","l1 = loss(Y_pred1, Y)\n","l2 = loss(Y_pred2, Y)\n","print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Batch Loss1:  0.4966 \n","Batch Loss2: 1.2389\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wnu5pNUDNikR","colab_type":"text"},"source":["## MNIST\n","\n","http://yann.lecun.com/exdb/mnist/\n","\n","http://cs231n.github.io/neural-networks-3/#sgd\n","\n","https://blog.csdn.net/u010089444/article/details/76725843\n","\n","https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms\n","\n","https://pytorch.org/docs/stable/nn.functional.html?highlight=functional%20relu#torch.nn.functional.relu\n","\n","https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU\n"]},{"cell_type":"code","metadata":{"id":"vdnAVsjSNikS","colab_type":"code","colab":{}},"source":["# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","from __future__ import print_function\n","from torch import nn, optim, cuda\n","from torch.utils import data\n","from torchvision import datasets, transforms  # transforms: common image transformations\n","import torch.nn.functional as F\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3eNZzyvNikZ","colab_type":"code","colab":{},"outputId":"f80febc7-1d72-45ee-c173-fb119c99c403"},"source":["# Training settings\n","batch_size = 64\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","# device = 'cpu'\n","print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training MNIST Model on cuda\n","============================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LvEXQjJnNikc","colab_type":"code","colab":{}},"source":["# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAxukUKkNikg","colab_type":"code","colab":{}},"source":["# Data Loader (Input Pipeline)\n","train_loader = data.DataLoader(dataset=train_dataset,\n","                               batch_size=batch_size,  # 64\n","                               shuffle=True)\n","\n","test_loader = data.DataLoader(dataset=test_dataset,\n","                              batch_size=batch_size,\n","                              shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UnUxuQWNikl","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(784, 520)\n","        self.l2 = nn.Linear(520, 320)\n","        self.l3 = nn.Linear(320, 240)\n","        self.l4 = nn.Linear(240, 120)\n","        self.l5 = nn.Linear(120, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n","        x = F.relu(self.l1(x))  # activation function\n","        x = F.relu(self.l2(x))\n","        x = F.relu(self.l3(x))\n","        x = F.relu(self.l4(x))\n","        return self.l5(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FlbubCuNikp","colab_type":"code","colab":{}},"source":["model = Net()\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","# SGD with momentum is method which helps accelerate gradients vectors in the right directions\n","# thus leading to faster converging\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN-0gyzJNikt","colab_type":"code","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)  # Cross Entropy Loss\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += criterion(output, target).item()\n","        # get the index of the max\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n","          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2tp_RZ6Niky","colab_type":"code","colab":{},"outputId":"4aa0ea54-caef-4329-d3b3-18cc57b13e16"},"source":["since = time.time()\n","for epoch in range(1, 10):\n","    epoch_start = time.time()\n","    train(epoch)\n","    m, s = divmod(time.time() - epoch_start, 60)\n","    print(f'Training time: {m:.0f}m {s:.0f}s')\n","    test()\n","    m, s = divmod(time.time() - epoch_start, 60)\n","    print(f'Testing time: {m:.0f}m {s:.0f}s')\n","\n","m, s = divmod(time.time() - since, 60)\n","print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.295890\n","Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.307435\n","Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.282576\n","Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.260816\n","Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.049404\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0227, Accuracy: 5633/10000 (56%)\n","Testing time: 0m 8s\n","Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.280056\n","Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.837074\n","Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.550821\n","Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.487448\n","Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.378238\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0068, Accuracy: 8741/10000 (87%)\n","Testing time: 0m 8s\n","Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.452543\n","Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.423461\n","Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.629864\n","Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.337531\n","Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.222861\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0047, Accuracy: 9119/10000 (91%)\n","Testing time: 0m 8s\n","Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.251477\n","Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.153046\n","Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.341543\n","Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.173304\n","Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.320120\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0035, Accuracy: 9320/10000 (93%)\n","Testing time: 0m 8s\n","Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.201681\n","Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.308669\n","Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.150764\n","Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.165662\n","Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.129517\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0028, Accuracy: 9472/10000 (94%)\n","Testing time: 0m 8s\n","Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.170759\n","Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.089639\n","Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.126345\n","Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.115561\n","Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.150021\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0022, Accuracy: 9571/10000 (95%)\n","Testing time: 0m 8s\n","Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.102386\n","Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.126275\n","Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.062763\n","Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.069370\n","Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.091822\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0019, Accuracy: 9647/10000 (96%)\n","Testing time: 0m 8s\n","Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.136333\n","Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.137285\n","Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.068795\n","Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.087561\n","Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.104135\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0018, Accuracy: 9645/10000 (96%)\n","Testing time: 0m 8s\n","Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.085021\n","Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.158782\n","Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.035798\n","Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.071912\n","Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.050345\n","Training time: 0m 7s\n","===========================\n","Test set: Average loss: 0.0016, Accuracy: 9692/10000 (96%)\n","Testing time: 0m 8s\n","Total Time: 1m 10s\n","Model was trained on cuda!\n"],"name":"stdout"}]}]}
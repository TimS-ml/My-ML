{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# DataLoader\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## go through dataset\n",
    "\n",
    "basically is a data viewer\n",
    "\n",
    "wrap in tensor in each epoach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import from_numpy, tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):  # Dataset is from torch too\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)  # how many subprocesses to use for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 5\n",
      "Epoch: 10\n",
      "Epoch: 15\n",
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-fe0dbbf85843>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs, labels = tensor(inputs), tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 5\n",
      "Epoch: 10\n",
      "Epoch: 15\n",
      "Epoch: 20\n"
     ]
    }
   ],
   "source": [
    "# nothing interesting, just go over the data 2 times\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = tensor(inputs), tensor(labels)\n",
    "        \n",
    "        # Run your training process\n",
    "        if i % 5 == 0:\n",
    "            print(f'Epoch: {i}')\n",
    "            # print(f'Inputs {inputs.data} | Labels {labels.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression\n",
    "\n",
    "https://pytorch.org/docs/stable/torch.html?highlight=from_numpy#torch.from_numpy\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html?highlight=optim%20sgd#torch.optim.SGD\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?highlight=nn%20bceloss#torch.nn.BCELoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(8, 6)\n",
    "        self.l2 = nn.Linear(6, 4)\n",
    "        self.l3 = nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch: 6 | Loss: 21.7653\n",
      "Epoch 1 | Batch: 12 | Loss: 19.8895\n",
      "Epoch 1 | Batch: 18 | Loss: 22.2112\n",
      "Epoch 1 | Batch: 24 | Loss: 15.4154\n",
      "Epoch 2 | Batch: 6 | Loss: 21.6797\n",
      "Epoch 2 | Batch: 12 | Loss: 23.5177\n",
      "Epoch 2 | Batch: 18 | Loss: 18.6194\n",
      "Epoch 2 | Batch: 24 | Loss: 14.7989\n"
     ]
    }
   ],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        if (i+1) % 6 == 0:\n",
    "            print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tensor, max\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1: 0.3567\n",
      "Loss2: 2.3026\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy example (np only)\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])  # small loss\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])  # large loss\n",
    "\n",
    "# calculate loss manually\n",
    "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
    "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1: 0.4170 \n",
      "PyTorch Loss2: 1.8406\n",
      "Y_pred1: 0\n",
      "Y_pred2: 1\n"
     ]
    }
   ],
   "source": [
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot!!!\n",
    "Y = tensor([0], requires_grad=False)  # means class 0 is correct\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)!!!\n",
    "Y_pred1 = tensor([[2.0, 1.0, 0.1]])  # should be small, since [0] is larger than the rest\n",
    "Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
    "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n",
    "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss1:  0.4966 \n",
      "Batch Loss2: 1.2389\n"
     ]
    }
   ],
   "source": [
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = tensor([2, 0, 1], requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
    "                  [1.1, 0.1, 0.2],\n",
    "                  [0.2, 2.1, 0.1]])  # should be small\n",
    "\n",
    "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
    "                  [0.2, 0.3, 0.5],\n",
    "                  [0.2, 0.2, 0.5]])  # should be large\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/#sgd\n",
    "\n",
    "https://blog.csdn.net/u010089444/article/details/76725843\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html?highlight=functional%20relu#torch.nn.functional.relu\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms  # transforms: common image transformations\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cuda\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                               batch_size=batch_size,  # 64\n",
    "                               shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))  # activation function\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# SGD with momentum is method which helps accelerate gradients vectors in the right directions\n",
    "# thus leading to faster converging\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)  # Cross Entropy Loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.295890\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.307435\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.282576\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.260816\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.049404\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0227, Accuracy: 5633/10000 (56%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.280056\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.837074\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.550821\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.487448\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.378238\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0068, Accuracy: 8741/10000 (87%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.452543\n",
      "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.423461\n",
      "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.629864\n",
      "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.337531\n",
      "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.222861\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0047, Accuracy: 9119/10000 (91%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.251477\n",
      "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.153046\n",
      "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.341543\n",
      "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.173304\n",
      "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.320120\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0035, Accuracy: 9320/10000 (93%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.201681\n",
      "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.308669\n",
      "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.150764\n",
      "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.165662\n",
      "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.129517\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0028, Accuracy: 9472/10000 (94%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.170759\n",
      "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.089639\n",
      "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.126345\n",
      "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.115561\n",
      "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.150021\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0022, Accuracy: 9571/10000 (95%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.102386\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.126275\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.062763\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.069370\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.091822\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0019, Accuracy: 9647/10000 (96%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.136333\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.137285\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.068795\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.087561\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.104135\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0018, Accuracy: 9645/10000 (96%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.085021\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.158782\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.035798\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.071912\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.050345\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0016, Accuracy: 9692/10000 (96%)\n",
      "Testing time: 0m 8s\n",
      "Total Time: 1m 10s\n",
      "Model was trained on cuda!\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(1, 10):\n",
    "    epoch_start = time.time()\n",
    "    train(epoch)\n",
    "    m, s = divmod(time.time() - epoch_start, 60)\n",
    "    print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "    test()\n",
    "    m, s = divmod(time.time() - epoch_start, 60)\n",
    "    print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "m, s = divmod(time.time() - since, 60)\n",
    "print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?highlight=maxpool2d#torch.nn.MaxPool2d\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)  # kernel size\n",
    "        self.fc = nn.Linear(320, 10)  # avoid size mismatch, in: 64 * 320\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))  # max pool, print x.size() to check out\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        # It is almost always you will need the last dimension when you compute the cross-entropy\n",
    "        return F.log_softmax(x, -1)  # add dim -1 to avoid warning...\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/two-small-questions-about-with-torch-no-grad/27571\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)  # change Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.059847\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.194853\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.057874\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.083139\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.073367\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.129999\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.144854\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.191636\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.069492\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.155353\n",
      "\n",
      "Test set: Average loss: 0.0745, Accuracy: 9758/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.132020\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.071940\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.172060\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.129315\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.113893\n",
      "\n",
      "Test set: Average loss: 0.0726, Accuracy: 9769/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.022187\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.052929\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.059229\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.152275\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.060212\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9796/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.105415\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.105495\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.024767\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.148450\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.022125\n",
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.029692\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.086405\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.059128\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.037345\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.008930\n",
      "\n",
      "Test set: Average loss: 0.0599, Accuracy: 9803/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.055871\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.187382\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.130143\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.030888\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.041143\n",
      "\n",
      "Test set: Average loss: 0.0517, Accuracy: 9835/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.027717\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.069186\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013716\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.010030\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.069819\n",
      "\n",
      "Test set: Average loss: 0.0528, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018547\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.044696\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.122486\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.058019\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.027669\n",
      "\n",
      "Test set: Average loss: 0.0512, Accuracy: 9830/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advance example\n",
    "\n",
    "In the context of convolutional neural networks, kernel = filter = feature detector\n",
    "\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "\n",
    "https://www.youtube.com/watch?v=VxhSouuSZDY\n",
    "\n",
    "https://buzzrobot.com/whats-happening-inside-the-convolutional-neural-network-the-answer-is-convolution-2c22075dc68d\n",
    "\n",
    "https://docs.python.org/3/library/argparse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse  # interesting package indeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make comparison of each convolution\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)  # inceptions\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        # Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
    "        return F.log_softmax(x, -1)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.321258\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.051745\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.210887\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.033756\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.062098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/miniconda3/envs/ptgpu/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0895, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.081887\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.198613\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.122356\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.120923\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.033388\n",
      "\n",
      "Test set: Average loss: 0.0791, Accuracy: 9754/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.055966\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.103866\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.036902\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.048040\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.288322\n",
      "\n",
      "Test set: Average loss: 0.0634, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.038738\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.077438\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.071596\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.026354\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
